{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 参考教程 https://jaketae.github.io/study/pytorch-rnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载训练语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/bn/tob-lq/qianweishuo/pytorchTutorial\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/bn/tob-lq/qianweishuo/pytorchTutorial'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /mnt/bn/tob-lq/qianweishuo/pytorchTutorial\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2814k  100 2814k    0     0  2907k      0 --:--:-- --:--:-- --:--:-- 2904k\n",
      "Archive:  name2lang.zip\n",
      "   creating: data/name2lang/data/\n",
      "  inflating: data/name2lang/data/eng-fra.txt  \n",
      "   creating: data/name2lang/data/names/\n",
      "  inflating: data/name2lang/data/names/Arabic.txt  \n",
      "  inflating: data/name2lang/data/names/Chinese.txt  \n",
      "  inflating: data/name2lang/data/names/Czech.txt  \n",
      "  inflating: data/name2lang/data/names/Dutch.txt  \n",
      "  inflating: data/name2lang/data/names/English.txt  \n",
      "  inflating: data/name2lang/data/names/French.txt  \n",
      "  inflating: data/name2lang/data/names/German.txt  \n",
      "  inflating: data/name2lang/data/names/Greek.txt  \n",
      "  inflating: data/name2lang/data/names/Irish.txt  \n",
      "  inflating: data/name2lang/data/names/Italian.txt  \n",
      "  inflating: data/name2lang/data/names/Japanese.txt  \n",
      "  inflating: data/name2lang/data/names/Korean.txt  \n",
      "  inflating: data/name2lang/data/names/Polish.txt  \n",
      "  inflating: data/name2lang/data/names/Portuguese.txt  \n",
      "  inflating: data/name2lang/data/names/Russian.txt  \n",
      "  inflating: data/name2lang/data/names/Scottish.txt  \n",
      "  inflating: data/name2lang/data/names/Spanish.txt  \n",
      "  inflating: data/name2lang/data/names/Vietnamese.txt  \n"
     ]
    }
   ],
   "source": [
    "!curl -o name2lang.zip https://download.pytorch.org/tutorial/data.zip\n",
    "!mkdir -p data/name2lang && unzip name2lang.zip -d data/name2lang\n",
    "!mv data/name2lang/data/* data/name2lang/\n",
    "!rm -rf ./data.zip data/name2lang/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lang2label, name2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import ascii_letters\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': tensor([0]),\n",
       " 'Chinese': tensor([1]),\n",
       " 'Czech': tensor([2]),\n",
       " 'Dutch': tensor([3]),\n",
       " 'English': tensor([4]),\n",
       " 'French': tensor([5]),\n",
       " 'German': tensor([6]),\n",
       " 'Greek': tensor([7]),\n",
       " 'Irish': tensor([8]),\n",
       " 'Italian': tensor([9]),\n",
       " 'Japanese': tensor([10]),\n",
       " 'Korean': tensor([11]),\n",
       " 'Polish': tensor([12]),\n",
       " 'Portuguese': tensor([13]),\n",
       " 'Russian': tensor([14]),\n",
       " 'Scottish': tensor([15]),\n",
       " 'Spanish': tensor([16]),\n",
       " 'Vietnamese': tensor([17])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"./data/name2lang/names\"\n",
    "lang2label = {file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long) for i, file_name in enumerate(os.listdir(data_dir))}\n",
    "lang2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "num_langs = len(lang2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx)\n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def name2tensor(name):\n",
    "    \"\"\"\n",
    "    In PyTorch, RNN layers expect the input tensor to be of size (seq_len, batch_size, input_size).\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(name), 1, num_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        tensor[i][0][char2idx[char]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2tensor(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20070, [tensor([0]), tensor([0]), tensor([0])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_names = []\n",
    "target_langs = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, file)) as f:\n",
    "        lang = file.split(\".\")[0]\n",
    "        names = [unidecode(line.rstrip()) for line in f]\n",
    "        for name in names:\n",
    "            try:\n",
    "                tensor_names.append(name2tensor(name))\n",
    "                target_langs.append(lang2label[lang])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "len(target_langs), target_langs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(target_langs)),\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    stratify=[e.item() for e in target_langs],  # 注意不是 stratify=target_langs,\n",
    ")\n",
    "train_dataset = [(tensor_names[i], target_langs[i]) for i in train_idx]\n",
    "test_dataset = [(tensor_names[i], target_langs[i]) for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18063\n",
      "Test: 2007\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat(tensors=(x, hidden_state), dim=1)  # (1, num_letters + hidden_size)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))  # (1, hidden_size)\n",
    "        output = self.in2output(combined)  # (1, num_langs)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n",
    "\n",
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MyRNN(input_size=num_letters, hidden_size=hidden_size, output_size=num_langs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262.61s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 19 23:29:52 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:60:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    66W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [30/18063], Loss: 0.0239\n",
      "Epoch [1/2], Step [60/18063], Loss: 0.1119\n",
      "Epoch [1/2], Step [90/18063], Loss: 0.1356\n",
      "Epoch [1/2], Step [120/18063], Loss: 3.0097\n",
      "Epoch [1/2], Step [150/18063], Loss: 0.0751\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)  \u001b[39m# logits v.s. target\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(parameters\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, norm_type\u001b[39m=\u001b[39m\u001b[39m2.0\u001b[39m)  \u001b[39m# Clips gradient norm of an iterable of parameters.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "print_interval = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_hidden()  # (1, hidden_size)\n",
    "        for char in name:  # (1, num_letters)\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        loss = criterion(output, label)  # logits v.s. target\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2.0)  # Clips gradient norm of an iterable of parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \" f\"Step [{i + 1}/{len(train_dataset)}], \" f\"Loss: {loss.item():.4f}\")\n",
    "        if i > 300: break  # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d43cbb2cc114a23a2e21c17a005e2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "逐条测试:   0%|          | 0/2007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.8859%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_correct = 0\n",
    "num_samples = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for name, label in tqdm(test_dataset, desc='逐条测试'):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian\n",
      "Russian\n",
      "Russian\n"
     ]
    }
   ],
   "source": [
    "label2lang = {label.item(): lang for lang, label in lang2label.items()}\n",
    "\n",
    "\n",
    "def myrnn_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in tensor_name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()\n",
    "    return label2lang[pred.item()]\n",
    "\n",
    "\n",
    "for name in ['Mike', 'Qin', 'Slaveya']:\n",
    "    print(myrnn_predict(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch GRU on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=num_letters,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_langs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_state = self.init_hidden()\n",
    "        output, hidden_state = self.gru(x, hidden_state)\n",
    "        output = self.fc(output[-1])\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "model = GRUModel(num_layers=2, hidden_size=hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001f4f7ada9c43dba2ea63ce3009f643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30916191c11442ba59b1fe0a975d779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [30/18063], Loss: 0.2804\n",
      "Epoch [1/2], Step [60/18063], Loss: 5.4819\n",
      "Epoch [1/2], Step [90/18063], Loss: 1.7093\n",
      "Epoch [1/2], Step [120/18063], Loss: 1.6225\n",
      "Epoch [1/2], Step [150/18063], Loss: 0.3381\n",
      "Epoch [1/2], Step [180/18063], Loss: 2.9073\n",
      "Epoch [1/2], Step [210/18063], Loss: 4.1206\n",
      "Epoch [1/2], Step [240/18063], Loss: 1.8794\n",
      "Epoch [1/2], Step [270/18063], Loss: 4.1809\n",
      "Epoch [1/2], Step [300/18063], Loss: 2.8752\n",
      "Epoch [1/2], Step [330/18063], Loss: 2.3095\n",
      "Epoch [1/2], Step [360/18063], Loss: 9.1140\n",
      "Epoch [1/2], Step [390/18063], Loss: 3.0998\n",
      "Epoch [1/2], Step [420/18063], Loss: 0.1925\n",
      "Epoch [1/2], Step [450/18063], Loss: 3.3917\n",
      "Epoch [1/2], Step [480/18063], Loss: 0.7813\n",
      "Epoch [1/2], Step [510/18063], Loss: 0.8431\n",
      "Epoch [1/2], Step [540/18063], Loss: 2.0390\n",
      "Epoch [1/2], Step [570/18063], Loss: 1.6805\n",
      "Epoch [1/2], Step [600/18063], Loss: 0.9868\n",
      "Epoch [1/2], Step [630/18063], Loss: 4.0865\n",
      "Epoch [1/2], Step [660/18063], Loss: 1.1449\n",
      "Epoch [1/2], Step [690/18063], Loss: 4.9542\n",
      "Epoch [1/2], Step [720/18063], Loss: 0.1313\n",
      "Epoch [1/2], Step [750/18063], Loss: 0.9074\n",
      "Epoch [1/2], Step [780/18063], Loss: 3.0744\n",
      "Epoch [1/2], Step [810/18063], Loss: 1.2309\n",
      "Epoch [1/2], Step [840/18063], Loss: 0.7735\n",
      "Epoch [1/2], Step [870/18063], Loss: 0.0396\n",
      "Epoch [1/2], Step [900/18063], Loss: 0.0091\n",
      "Epoch [1/2], Step [930/18063], Loss: 0.0010\n",
      "Epoch [1/2], Step [960/18063], Loss: 2.4673\n",
      "Epoch [1/2], Step [990/18063], Loss: 1.8507\n",
      "Epoch [1/2], Step [1020/18063], Loss: 0.4320\n",
      "Epoch [1/2], Step [1050/18063], Loss: 0.1224\n",
      "Epoch [1/2], Step [1080/18063], Loss: 3.2515\n",
      "Epoch [1/2], Step [1110/18063], Loss: 1.0518\n",
      "Epoch [1/2], Step [1140/18063], Loss: 1.2882\n",
      "Epoch [1/2], Step [1170/18063], Loss: 0.6228\n",
      "Epoch [1/2], Step [1200/18063], Loss: 0.6992\n",
      "Epoch [1/2], Step [1230/18063], Loss: 3.1059\n",
      "Epoch [1/2], Step [1260/18063], Loss: 1.8746\n",
      "Epoch [1/2], Step [1290/18063], Loss: 3.1944\n",
      "Epoch [1/2], Step [1320/18063], Loss: 1.3507\n",
      "Epoch [1/2], Step [1350/18063], Loss: 0.0570\n",
      "Epoch [1/2], Step [1380/18063], Loss: 2.2531\n",
      "Epoch [1/2], Step [1410/18063], Loss: 0.0767\n",
      "Epoch [1/2], Step [1440/18063], Loss: 0.0071\n",
      "Epoch [1/2], Step [1470/18063], Loss: 0.6056\n",
      "Epoch [1/2], Step [1500/18063], Loss: 0.0033\n",
      "Epoch [1/2], Step [1530/18063], Loss: 2.5083\n",
      "Epoch [1/2], Step [1560/18063], Loss: 2.7980\n",
      "Epoch [1/2], Step [1590/18063], Loss: 4.9239\n",
      "Epoch [1/2], Step [1620/18063], Loss: 1.0945\n",
      "Epoch [1/2], Step [1650/18063], Loss: 0.9584\n",
      "Epoch [1/2], Step [1680/18063], Loss: 1.3336\n",
      "Epoch [1/2], Step [1710/18063], Loss: 0.0648\n",
      "Epoch [1/2], Step [1740/18063], Loss: 1.2566\n",
      "Epoch [1/2], Step [1770/18063], Loss: 0.7624\n",
      "Epoch [1/2], Step [1800/18063], Loss: 2.1393\n",
      "Epoch [1/2], Step [1830/18063], Loss: 0.0330\n",
      "Epoch [1/2], Step [1860/18063], Loss: 1.6256\n",
      "Epoch [1/2], Step [1890/18063], Loss: 1.1761\n",
      "Epoch [1/2], Step [1920/18063], Loss: 0.5596\n",
      "Epoch [1/2], Step [1950/18063], Loss: 0.4714\n",
      "Epoch [1/2], Step [1980/18063], Loss: 0.2985\n",
      "Epoch [1/2], Step [2010/18063], Loss: 0.0106\n",
      "Epoch [1/2], Step [2040/18063], Loss: 0.0231\n",
      "Epoch [1/2], Step [2070/18063], Loss: 2.8551\n",
      "Epoch [1/2], Step [2100/18063], Loss: 1.1804\n",
      "Epoch [1/2], Step [2130/18063], Loss: 0.5685\n",
      "Epoch [1/2], Step [2160/18063], Loss: 1.1132\n",
      "Epoch [1/2], Step [2190/18063], Loss: 0.1811\n",
      "Epoch [1/2], Step [2220/18063], Loss: 0.3600\n",
      "Epoch [1/2], Step [2250/18063], Loss: 0.0068\n",
      "Epoch [1/2], Step [2280/18063], Loss: 0.3879\n",
      "Epoch [1/2], Step [2310/18063], Loss: 0.0261\n",
      "Epoch [1/2], Step [2340/18063], Loss: 0.0365\n",
      "Epoch [1/2], Step [2370/18063], Loss: 2.4931\n",
      "Epoch [1/2], Step [2400/18063], Loss: 3.1654\n",
      "Epoch [1/2], Step [2430/18063], Loss: 0.0269\n",
      "Epoch [1/2], Step [2460/18063], Loss: 0.0365\n",
      "Epoch [1/2], Step [2490/18063], Loss: 0.4234\n",
      "Epoch [1/2], Step [2520/18063], Loss: 0.0118\n",
      "Epoch [1/2], Step [2550/18063], Loss: 0.0086\n",
      "Epoch [1/2], Step [2580/18063], Loss: 2.4555\n",
      "Epoch [1/2], Step [2610/18063], Loss: 2.0590\n",
      "Epoch [1/2], Step [2640/18063], Loss: 0.9960\n",
      "Epoch [1/2], Step [2670/18063], Loss: 0.0023\n",
      "Epoch [1/2], Step [2700/18063], Loss: 0.0984\n",
      "Epoch [1/2], Step [2730/18063], Loss: 0.0027\n",
      "Epoch [1/2], Step [2760/18063], Loss: 0.2184\n",
      "Epoch [1/2], Step [2790/18063], Loss: 0.0749\n",
      "Epoch [1/2], Step [2820/18063], Loss: 0.0240\n",
      "Epoch [1/2], Step [2850/18063], Loss: 0.5075\n",
      "Epoch [1/2], Step [2880/18063], Loss: 0.3424\n",
      "Epoch [1/2], Step [2910/18063], Loss: 0.0041\n",
      "Epoch [1/2], Step [2940/18063], Loss: 0.4328\n",
      "Epoch [1/2], Step [2970/18063], Loss: 0.0094\n",
      "Epoch [1/2], Step [3000/18063], Loss: 1.3119\n",
      "Epoch [1/2], Step [3030/18063], Loss: 0.0120\n",
      "Epoch [1/2], Step [3060/18063], Loss: 0.2701\n",
      "Epoch [1/2], Step [3090/18063], Loss: 1.9209\n",
      "Epoch [1/2], Step [3120/18063], Loss: 0.2354\n",
      "Epoch [1/2], Step [3150/18063], Loss: 0.0048\n",
      "Epoch [1/2], Step [3180/18063], Loss: 0.0723\n",
      "Epoch [1/2], Step [3210/18063], Loss: 0.0032\n",
      "Epoch [1/2], Step [3240/18063], Loss: 0.2908\n",
      "Epoch [1/2], Step [3270/18063], Loss: 0.0003\n",
      "Epoch [1/2], Step [3300/18063], Loss: 2.2867\n",
      "Epoch [1/2], Step [3330/18063], Loss: 0.0028\n",
      "Epoch [1/2], Step [3360/18063], Loss: 4.0505\n",
      "Epoch [1/2], Step [3390/18063], Loss: 0.0378\n",
      "Epoch [1/2], Step [3420/18063], Loss: 0.3816\n",
      "Epoch [1/2], Step [3450/18063], Loss: 0.0014\n",
      "Epoch [1/2], Step [3480/18063], Loss: 1.7116\n",
      "Epoch [1/2], Step [3510/18063], Loss: 2.9332\n",
      "Epoch [1/2], Step [3540/18063], Loss: 1.8214\n",
      "Epoch [1/2], Step [3570/18063], Loss: 0.0120\n",
      "Epoch [1/2], Step [3600/18063], Loss: 0.3780\n",
      "Epoch [1/2], Step [3630/18063], Loss: 0.0023\n",
      "Epoch [1/2], Step [3660/18063], Loss: 3.0246\n",
      "Epoch [1/2], Step [3690/18063], Loss: 0.0026\n",
      "Epoch [1/2], Step [3720/18063], Loss: 0.0523\n",
      "Epoch [1/2], Step [3750/18063], Loss: 0.3406\n",
      "Epoch [1/2], Step [3780/18063], Loss: 3.1225\n",
      "Epoch [1/2], Step [3810/18063], Loss: 2.8056\n",
      "Epoch [1/2], Step [3840/18063], Loss: 2.9667\n",
      "Epoch [1/2], Step [3870/18063], Loss: 1.5927\n",
      "Epoch [1/2], Step [3900/18063], Loss: 5.4768\n",
      "Epoch [1/2], Step [3930/18063], Loss: 0.6568\n",
      "Epoch [1/2], Step [3960/18063], Loss: 0.5775\n",
      "Epoch [1/2], Step [3990/18063], Loss: 0.4801\n",
      "Epoch [1/2], Step [4020/18063], Loss: 0.0047\n",
      "Epoch [1/2], Step [4050/18063], Loss: 0.0345\n",
      "Epoch [1/2], Step [4080/18063], Loss: 0.1153\n",
      "Epoch [1/2], Step [4110/18063], Loss: 1.2417\n",
      "Epoch [1/2], Step [4140/18063], Loss: 0.0250\n",
      "Epoch [1/2], Step [4170/18063], Loss: 2.0859\n",
      "Epoch [1/2], Step [4200/18063], Loss: 0.0013\n",
      "Epoch [1/2], Step [4230/18063], Loss: 3.4136\n",
      "Epoch [1/2], Step [4260/18063], Loss: 0.6088\n",
      "Epoch [1/2], Step [4290/18063], Loss: 0.0693\n",
      "Epoch [1/2], Step [4320/18063], Loss: 4.9714\n",
      "Epoch [1/2], Step [4350/18063], Loss: 1.6310\n",
      "Epoch [1/2], Step [4380/18063], Loss: 0.0314\n",
      "Epoch [1/2], Step [4410/18063], Loss: 3.7116\n",
      "Epoch [1/2], Step [4440/18063], Loss: 1.6803\n",
      "Epoch [1/2], Step [4470/18063], Loss: 0.0532\n",
      "Epoch [1/2], Step [4500/18063], Loss: 1.8531\n",
      "Epoch [1/2], Step [4530/18063], Loss: 0.1383\n",
      "Epoch [1/2], Step [4560/18063], Loss: 1.1194\n",
      "Epoch [1/2], Step [4590/18063], Loss: 1.2251\n",
      "Epoch [1/2], Step [4620/18063], Loss: 4.1289\n",
      "Epoch [1/2], Step [4650/18063], Loss: 0.4241\n",
      "Epoch [1/2], Step [4680/18063], Loss: 0.1122\n",
      "Epoch [1/2], Step [4710/18063], Loss: 0.0030\n",
      "Epoch [1/2], Step [4740/18063], Loss: 1.6910\n",
      "Epoch [1/2], Step [4770/18063], Loss: 2.7758\n",
      "Epoch [1/2], Step [4800/18063], Loss: 0.0180\n",
      "Epoch [1/2], Step [4830/18063], Loss: 0.0027\n",
      "Epoch [1/2], Step [4860/18063], Loss: 1.2899\n",
      "Epoch [1/2], Step [4890/18063], Loss: 2.1029\n",
      "Epoch [1/2], Step [4920/18063], Loss: 3.0265\n",
      "Epoch [1/2], Step [4950/18063], Loss: 0.0090\n",
      "Epoch [1/2], Step [4980/18063], Loss: 0.3720\n",
      "Epoch [1/2], Step [5010/18063], Loss: 0.0029\n",
      "Epoch [1/2], Step [5040/18063], Loss: 0.8307\n",
      "Epoch [1/2], Step [5070/18063], Loss: 0.0060\n",
      "Epoch [1/2], Step [5100/18063], Loss: 1.8137\n",
      "Epoch [1/2], Step [5130/18063], Loss: 3.5180\n",
      "Epoch [1/2], Step [5160/18063], Loss: 1.0181\n",
      "Epoch [1/2], Step [5190/18063], Loss: 2.1999\n",
      "Epoch [1/2], Step [5220/18063], Loss: 0.0003\n",
      "Epoch [1/2], Step [5250/18063], Loss: 0.0977\n",
      "Epoch [1/2], Step [5280/18063], Loss: 2.9369\n",
      "Epoch [1/2], Step [5310/18063], Loss: 0.0704\n",
      "Epoch [1/2], Step [5340/18063], Loss: 0.7733\n",
      "Epoch [1/2], Step [5370/18063], Loss: 1.0469\n",
      "Epoch [1/2], Step [5400/18063], Loss: 0.0036\n",
      "Epoch [1/2], Step [5430/18063], Loss: 3.5812\n",
      "Epoch [1/2], Step [5460/18063], Loss: 0.0885\n",
      "Epoch [1/2], Step [5490/18063], Loss: 0.0442\n",
      "Epoch [1/2], Step [5520/18063], Loss: 0.0079\n",
      "Epoch [1/2], Step [5550/18063], Loss: 0.7144\n",
      "Epoch [1/2], Step [5580/18063], Loss: 0.0547\n",
      "Epoch [1/2], Step [5610/18063], Loss: 0.0035\n",
      "Epoch [1/2], Step [5640/18063], Loss: 0.7273\n",
      "Epoch [1/2], Step [5670/18063], Loss: 3.7916\n",
      "Epoch [1/2], Step [5700/18063], Loss: 2.2350\n",
      "Epoch [1/2], Step [5730/18063], Loss: 0.0029\n",
      "Epoch [1/2], Step [5760/18063], Loss: 0.0006\n",
      "Epoch [1/2], Step [5790/18063], Loss: 3.6937\n",
      "Epoch [1/2], Step [5820/18063], Loss: 0.4869\n",
      "Epoch [1/2], Step [5850/18063], Loss: 0.6638\n",
      "Epoch [1/2], Step [5880/18063], Loss: 0.4624\n",
      "Epoch [1/2], Step [5910/18063], Loss: 0.0043\n",
      "Epoch [1/2], Step [5940/18063], Loss: 0.1788\n",
      "Epoch [1/2], Step [5970/18063], Loss: 1.7677\n",
      "Epoch [1/2], Step [6000/18063], Loss: 0.0485\n",
      "Epoch [1/2], Step [6030/18063], Loss: 0.3084\n",
      "Epoch [1/2], Step [6060/18063], Loss: 3.0052\n",
      "Epoch [1/2], Step [6090/18063], Loss: 0.0008\n",
      "Epoch [1/2], Step [6120/18063], Loss: 0.5942\n",
      "Epoch [1/2], Step [6150/18063], Loss: 0.0616\n",
      "Epoch [1/2], Step [6180/18063], Loss: 0.0340\n",
      "Epoch [1/2], Step [6210/18063], Loss: 1.3903\n",
      "Epoch [1/2], Step [6240/18063], Loss: 0.2299\n",
      "Epoch [1/2], Step [6270/18063], Loss: 3.3115\n",
      "Epoch [1/2], Step [6300/18063], Loss: 0.0140\n",
      "Epoch [1/2], Step [6330/18063], Loss: 4.1213\n",
      "Epoch [1/2], Step [6360/18063], Loss: 0.0148\n",
      "Epoch [1/2], Step [6390/18063], Loss: 0.0014\n",
      "Epoch [1/2], Step [6420/18063], Loss: 2.5096\n",
      "Epoch [1/2], Step [6450/18063], Loss: 0.0056\n",
      "Epoch [1/2], Step [6480/18063], Loss: 1.7379\n",
      "Epoch [1/2], Step [6510/18063], Loss: 0.0186\n",
      "Epoch [1/2], Step [6540/18063], Loss: 0.0985\n",
      "Epoch [1/2], Step [6570/18063], Loss: 0.9462\n",
      "Epoch [1/2], Step [6600/18063], Loss: 0.2101\n",
      "Epoch [1/2], Step [6630/18063], Loss: 0.0037\n",
      "Epoch [1/2], Step [6660/18063], Loss: 0.0023\n",
      "Epoch [1/2], Step [6690/18063], Loss: 0.1312\n",
      "Epoch [1/2], Step [6720/18063], Loss: 0.0080\n",
      "Epoch [1/2], Step [6750/18063], Loss: 2.5315\n",
      "Epoch [1/2], Step [6780/18063], Loss: 3.5660\n",
      "Epoch [1/2], Step [6810/18063], Loss: 0.0009\n",
      "Epoch [1/2], Step [6840/18063], Loss: 0.9075\n",
      "Epoch [1/2], Step [6870/18063], Loss: 2.3828\n",
      "Epoch [1/2], Step [6900/18063], Loss: 0.5592\n",
      "Epoch [1/2], Step [6930/18063], Loss: 0.0127\n",
      "Epoch [1/2], Step [6960/18063], Loss: 0.0136\n",
      "Epoch [1/2], Step [6990/18063], Loss: 0.5191\n",
      "Epoch [1/2], Step [7020/18063], Loss: 1.1291\n",
      "Epoch [1/2], Step [7050/18063], Loss: 0.0182\n",
      "Epoch [1/2], Step [7080/18063], Loss: 0.2314\n",
      "Epoch [1/2], Step [7110/18063], Loss: 0.0583\n",
      "Epoch [1/2], Step [7140/18063], Loss: 0.5645\n",
      "Epoch [1/2], Step [7170/18063], Loss: 3.8304\n",
      "Epoch [1/2], Step [7200/18063], Loss: 0.2698\n",
      "Epoch [1/2], Step [7230/18063], Loss: 0.1417\n",
      "Epoch [1/2], Step [7260/18063], Loss: 0.0252\n",
      "Epoch [1/2], Step [7290/18063], Loss: 0.0282\n",
      "Epoch [1/2], Step [7320/18063], Loss: 0.0113\n",
      "Epoch [1/2], Step [7350/18063], Loss: 0.0004\n",
      "Epoch [1/2], Step [7380/18063], Loss: 0.5937\n",
      "Epoch [1/2], Step [7410/18063], Loss: 0.0211\n",
      "Epoch [1/2], Step [7440/18063], Loss: 1.2801\n",
      "Epoch [1/2], Step [7470/18063], Loss: 0.0031\n",
      "Epoch [1/2], Step [7500/18063], Loss: 0.0004\n",
      "Epoch [1/2], Step [7530/18063], Loss: 0.0636\n",
      "Epoch [1/2], Step [7560/18063], Loss: 0.1363\n",
      "Epoch [1/2], Step [7590/18063], Loss: 1.7090\n",
      "Epoch [1/2], Step [7620/18063], Loss: 0.0601\n",
      "Epoch [1/2], Step [7650/18063], Loss: 1.6176\n",
      "Epoch [1/2], Step [7680/18063], Loss: 0.4975\n",
      "Epoch [1/2], Step [7710/18063], Loss: 1.6495\n",
      "Epoch [1/2], Step [7740/18063], Loss: 0.0009\n",
      "Epoch [1/2], Step [7770/18063], Loss: 0.2258\n",
      "Epoch [1/2], Step [7800/18063], Loss: 0.2981\n",
      "Epoch [1/2], Step [7830/18063], Loss: 0.0013\n",
      "Epoch [1/2], Step [7860/18063], Loss: 0.0021\n",
      "Epoch [1/2], Step [7890/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [7920/18063], Loss: 0.0676\n",
      "Epoch [1/2], Step [7950/18063], Loss: 0.4288\n",
      "Epoch [1/2], Step [7980/18063], Loss: 0.2430\n",
      "Epoch [1/2], Step [8010/18063], Loss: 0.1518\n",
      "Epoch [1/2], Step [8040/18063], Loss: 0.0073\n",
      "Epoch [1/2], Step [8070/18063], Loss: 0.1297\n",
      "Epoch [1/2], Step [8100/18063], Loss: 1.0020\n",
      "Epoch [1/2], Step [8130/18063], Loss: 0.2195\n",
      "Epoch [1/2], Step [8160/18063], Loss: 1.2563\n",
      "Epoch [1/2], Step [8190/18063], Loss: 0.0107\n",
      "Epoch [1/2], Step [8220/18063], Loss: 1.4099\n",
      "Epoch [1/2], Step [8250/18063], Loss: 0.1364\n",
      "Epoch [1/2], Step [8280/18063], Loss: 1.1308\n",
      "Epoch [1/2], Step [8310/18063], Loss: 3.1654\n",
      "Epoch [1/2], Step [8340/18063], Loss: 0.0192\n",
      "Epoch [1/2], Step [8370/18063], Loss: 0.0065\n",
      "Epoch [1/2], Step [8400/18063], Loss: 0.7237\n",
      "Epoch [1/2], Step [8430/18063], Loss: 1.2022\n",
      "Epoch [1/2], Step [8460/18063], Loss: 0.7997\n",
      "Epoch [1/2], Step [8490/18063], Loss: 1.1308\n",
      "Epoch [1/2], Step [8520/18063], Loss: 0.0004\n",
      "Epoch [1/2], Step [8550/18063], Loss: 1.3765\n",
      "Epoch [1/2], Step [8580/18063], Loss: 0.0117\n",
      "Epoch [1/2], Step [8610/18063], Loss: 0.2979\n",
      "Epoch [1/2], Step [8640/18063], Loss: 3.4642\n",
      "Epoch [1/2], Step [8670/18063], Loss: 3.9730\n",
      "Epoch [1/2], Step [8700/18063], Loss: 0.0220\n",
      "Epoch [1/2], Step [8730/18063], Loss: 3.3551\n",
      "Epoch [1/2], Step [8760/18063], Loss: 0.1002\n",
      "Epoch [1/2], Step [8790/18063], Loss: 0.0093\n",
      "Epoch [1/2], Step [8820/18063], Loss: 0.0024\n",
      "Epoch [1/2], Step [8850/18063], Loss: 0.4773\n",
      "Epoch [1/2], Step [8880/18063], Loss: 0.0109\n",
      "Epoch [1/2], Step [8910/18063], Loss: 0.0018\n",
      "Epoch [1/2], Step [8940/18063], Loss: 0.7862\n",
      "Epoch [1/2], Step [8970/18063], Loss: 0.4223\n",
      "Epoch [1/2], Step [9000/18063], Loss: 0.4785\n",
      "Epoch [1/2], Step [9030/18063], Loss: 0.6444\n",
      "Epoch [1/2], Step [9060/18063], Loss: 3.7417\n",
      "Epoch [1/2], Step [9090/18063], Loss: 0.1648\n",
      "Epoch [1/2], Step [9120/18063], Loss: 0.7010\n",
      "Epoch [1/2], Step [9150/18063], Loss: 3.9054\n",
      "Epoch [1/2], Step [9180/18063], Loss: 0.4997\n",
      "Epoch [1/2], Step [9210/18063], Loss: 0.0006\n",
      "Epoch [1/2], Step [9240/18063], Loss: 0.4592\n",
      "Epoch [1/2], Step [9270/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [9300/18063], Loss: 0.1516\n",
      "Epoch [1/2], Step [9330/18063], Loss: 2.4533\n",
      "Epoch [1/2], Step [9360/18063], Loss: 0.1026\n",
      "Epoch [1/2], Step [9390/18063], Loss: 0.0012\n",
      "Epoch [1/2], Step [9420/18063], Loss: 2.8946\n",
      "Epoch [1/2], Step [9450/18063], Loss: 0.6814\n",
      "Epoch [1/2], Step [9480/18063], Loss: 1.4759\n",
      "Epoch [1/2], Step [9510/18063], Loss: 2.0501\n",
      "Epoch [1/2], Step [9540/18063], Loss: 0.5790\n",
      "Epoch [1/2], Step [9570/18063], Loss: 0.2206\n",
      "Epoch [1/2], Step [9600/18063], Loss: 2.9362\n",
      "Epoch [1/2], Step [9630/18063], Loss: 0.6129\n",
      "Epoch [1/2], Step [9660/18063], Loss: 2.5558\n",
      "Epoch [1/2], Step [9690/18063], Loss: 0.3616\n",
      "Epoch [1/2], Step [9720/18063], Loss: 3.5534\n",
      "Epoch [1/2], Step [9750/18063], Loss: 0.0049\n",
      "Epoch [1/2], Step [9780/18063], Loss: 0.0041\n",
      "Epoch [1/2], Step [9810/18063], Loss: 0.2800\n",
      "Epoch [1/2], Step [9840/18063], Loss: 0.3812\n",
      "Epoch [1/2], Step [9870/18063], Loss: 1.0401\n",
      "Epoch [1/2], Step [9900/18063], Loss: 2.2689\n",
      "Epoch [1/2], Step [9930/18063], Loss: 2.9251\n",
      "Epoch [1/2], Step [9960/18063], Loss: 0.0019\n",
      "Epoch [1/2], Step [9990/18063], Loss: 2.6167\n",
      "Epoch [1/2], Step [10020/18063], Loss: 0.0040\n",
      "Epoch [1/2], Step [10050/18063], Loss: 1.8842\n",
      "Epoch [1/2], Step [10080/18063], Loss: 0.3871\n",
      "Epoch [1/2], Step [10110/18063], Loss: 2.7001\n",
      "Epoch [1/2], Step [10140/18063], Loss: 1.1423\n",
      "Epoch [1/2], Step [10170/18063], Loss: 0.6612\n",
      "Epoch [1/2], Step [10200/18063], Loss: 0.0015\n",
      "Epoch [1/2], Step [10230/18063], Loss: 1.5454\n",
      "Epoch [1/2], Step [10260/18063], Loss: 0.0035\n",
      "Epoch [1/2], Step [10290/18063], Loss: 1.3614\n",
      "Epoch [1/2], Step [10320/18063], Loss: 0.5718\n",
      "Epoch [1/2], Step [10350/18063], Loss: 2.4611\n",
      "Epoch [1/2], Step [10380/18063], Loss: 0.4835\n",
      "Epoch [1/2], Step [10410/18063], Loss: 0.0052\n",
      "Epoch [1/2], Step [10440/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [10470/18063], Loss: 2.0591\n",
      "Epoch [1/2], Step [10500/18063], Loss: 2.8836\n",
      "Epoch [1/2], Step [10530/18063], Loss: 0.0170\n",
      "Epoch [1/2], Step [10560/18063], Loss: 0.8164\n",
      "Epoch [1/2], Step [10590/18063], Loss: 3.1227\n",
      "Epoch [1/2], Step [10620/18063], Loss: 0.2312\n",
      "Epoch [1/2], Step [10650/18063], Loss: 0.5138\n",
      "Epoch [1/2], Step [10680/18063], Loss: 0.0487\n",
      "Epoch [1/2], Step [10710/18063], Loss: 0.4186\n",
      "Epoch [1/2], Step [10740/18063], Loss: 0.2883\n",
      "Epoch [1/2], Step [10770/18063], Loss: 1.1383\n",
      "Epoch [1/2], Step [10800/18063], Loss: 0.0196\n",
      "Epoch [1/2], Step [10830/18063], Loss: 4.1047\n",
      "Epoch [1/2], Step [10860/18063], Loss: 0.0379\n",
      "Epoch [1/2], Step [10890/18063], Loss: 0.2483\n",
      "Epoch [1/2], Step [10920/18063], Loss: 0.0212\n",
      "Epoch [1/2], Step [10950/18063], Loss: 4.2930\n",
      "Epoch [1/2], Step [10980/18063], Loss: 0.1672\n",
      "Epoch [1/2], Step [11010/18063], Loss: 2.6646\n",
      "Epoch [1/2], Step [11040/18063], Loss: 0.3149\n",
      "Epoch [1/2], Step [11070/18063], Loss: 0.0058\n",
      "Epoch [1/2], Step [11100/18063], Loss: 0.0110\n",
      "Epoch [1/2], Step [11130/18063], Loss: 0.0757\n",
      "Epoch [1/2], Step [11160/18063], Loss: 1.2142\n",
      "Epoch [1/2], Step [11190/18063], Loss: 1.9757\n",
      "Epoch [1/2], Step [11220/18063], Loss: 2.9575\n",
      "Epoch [1/2], Step [11250/18063], Loss: 0.4645\n",
      "Epoch [1/2], Step [11280/18063], Loss: 0.0107\n",
      "Epoch [1/2], Step [11310/18063], Loss: 0.0036\n",
      "Epoch [1/2], Step [11340/18063], Loss: 0.2098\n",
      "Epoch [1/2], Step [11370/18063], Loss: 0.0022\n",
      "Epoch [1/2], Step [11400/18063], Loss: 0.0004\n",
      "Epoch [1/2], Step [11430/18063], Loss: 0.0214\n",
      "Epoch [1/2], Step [11460/18063], Loss: 0.6220\n",
      "Epoch [1/2], Step [11490/18063], Loss: 0.4944\n",
      "Epoch [1/2], Step [11520/18063], Loss: 4.2705\n",
      "Epoch [1/2], Step [11550/18063], Loss: 0.0127\n",
      "Epoch [1/2], Step [11580/18063], Loss: 0.7862\n",
      "Epoch [1/2], Step [11610/18063], Loss: 0.1534\n",
      "Epoch [1/2], Step [11640/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [11670/18063], Loss: 0.0171\n",
      "Epoch [1/2], Step [11700/18063], Loss: 1.8274\n",
      "Epoch [1/2], Step [11730/18063], Loss: 4.0547\n",
      "Epoch [1/2], Step [11760/18063], Loss: 5.6593\n",
      "Epoch [1/2], Step [11790/18063], Loss: 1.6217\n",
      "Epoch [1/2], Step [11820/18063], Loss: 0.0011\n",
      "Epoch [1/2], Step [11850/18063], Loss: 1.7882\n",
      "Epoch [1/2], Step [11880/18063], Loss: 2.4173\n",
      "Epoch [1/2], Step [11910/18063], Loss: 0.0045\n",
      "Epoch [1/2], Step [11940/18063], Loss: 0.9113\n",
      "Epoch [1/2], Step [11970/18063], Loss: 3.4337\n",
      "Epoch [1/2], Step [12000/18063], Loss: 0.2307\n",
      "Epoch [1/2], Step [12030/18063], Loss: 0.2871\n",
      "Epoch [1/2], Step [12060/18063], Loss: 0.3263\n",
      "Epoch [1/2], Step [12090/18063], Loss: 0.0022\n",
      "Epoch [1/2], Step [12120/18063], Loss: 0.0901\n",
      "Epoch [1/2], Step [12150/18063], Loss: 0.0085\n",
      "Epoch [1/2], Step [12180/18063], Loss: 2.2407\n",
      "Epoch [1/2], Step [12210/18063], Loss: 4.7970\n",
      "Epoch [1/2], Step [12240/18063], Loss: 0.0129\n",
      "Epoch [1/2], Step [12270/18063], Loss: 0.5330\n",
      "Epoch [1/2], Step [12300/18063], Loss: 0.0091\n",
      "Epoch [1/2], Step [12330/18063], Loss: 0.3121\n",
      "Epoch [1/2], Step [12360/18063], Loss: 2.4950\n",
      "Epoch [1/2], Step [12390/18063], Loss: 0.0003\n",
      "Epoch [1/2], Step [12420/18063], Loss: 1.6632\n",
      "Epoch [1/2], Step [12450/18063], Loss: 0.0714\n",
      "Epoch [1/2], Step [12480/18063], Loss: 0.3945\n",
      "Epoch [1/2], Step [12510/18063], Loss: 2.5176\n",
      "Epoch [1/2], Step [12540/18063], Loss: 2.1526\n",
      "Epoch [1/2], Step [12570/18063], Loss: 0.0017\n",
      "Epoch [1/2], Step [12600/18063], Loss: 0.0023\n",
      "Epoch [1/2], Step [12630/18063], Loss: 0.1399\n",
      "Epoch [1/2], Step [12660/18063], Loss: 0.8768\n",
      "Epoch [1/2], Step [12690/18063], Loss: 2.1472\n",
      "Epoch [1/2], Step [12720/18063], Loss: 0.0750\n",
      "Epoch [1/2], Step [12750/18063], Loss: 0.0913\n",
      "Epoch [1/2], Step [12780/18063], Loss: 0.0015\n",
      "Epoch [1/2], Step [12810/18063], Loss: 3.8228\n",
      "Epoch [1/2], Step [12840/18063], Loss: 0.0023\n",
      "Epoch [1/2], Step [12870/18063], Loss: 0.0039\n",
      "Epoch [1/2], Step [12900/18063], Loss: 0.0008\n",
      "Epoch [1/2], Step [12930/18063], Loss: 0.0152\n",
      "Epoch [1/2], Step [12960/18063], Loss: 7.0296\n",
      "Epoch [1/2], Step [12990/18063], Loss: 0.0008\n",
      "Epoch [1/2], Step [13020/18063], Loss: 0.0010\n",
      "Epoch [1/2], Step [13050/18063], Loss: 0.5728\n",
      "Epoch [1/2], Step [13080/18063], Loss: 1.2705\n",
      "Epoch [1/2], Step [13110/18063], Loss: 0.1650\n",
      "Epoch [1/2], Step [13140/18063], Loss: 0.6560\n",
      "Epoch [1/2], Step [13170/18063], Loss: 3.8874\n",
      "Epoch [1/2], Step [13200/18063], Loss: 0.0013\n",
      "Epoch [1/2], Step [13230/18063], Loss: 0.1569\n",
      "Epoch [1/2], Step [13260/18063], Loss: 0.0062\n",
      "Epoch [1/2], Step [13290/18063], Loss: 0.1665\n",
      "Epoch [1/2], Step [13320/18063], Loss: 0.0203\n",
      "Epoch [1/2], Step [13350/18063], Loss: 0.0035\n",
      "Epoch [1/2], Step [13380/18063], Loss: 4.9309\n",
      "Epoch [1/2], Step [13410/18063], Loss: 0.3392\n",
      "Epoch [1/2], Step [13440/18063], Loss: 0.2363\n",
      "Epoch [1/2], Step [13470/18063], Loss: 0.0019\n",
      "Epoch [1/2], Step [13500/18063], Loss: 2.0258\n",
      "Epoch [1/2], Step [13530/18063], Loss: 0.7071\n",
      "Epoch [1/2], Step [13560/18063], Loss: 0.6439\n",
      "Epoch [1/2], Step [13590/18063], Loss: 5.1081\n",
      "Epoch [1/2], Step [13620/18063], Loss: 1.7884\n",
      "Epoch [1/2], Step [13650/18063], Loss: 0.0009\n",
      "Epoch [1/2], Step [13680/18063], Loss: 0.0004\n",
      "Epoch [1/2], Step [13710/18063], Loss: 0.0784\n",
      "Epoch [1/2], Step [13740/18063], Loss: 0.3165\n",
      "Epoch [1/2], Step [13770/18063], Loss: 2.1888\n",
      "Epoch [1/2], Step [13800/18063], Loss: 1.2951\n",
      "Epoch [1/2], Step [13830/18063], Loss: 0.0539\n",
      "Epoch [1/2], Step [13860/18063], Loss: 2.4515\n",
      "Epoch [1/2], Step [13890/18063], Loss: 0.0214\n",
      "Epoch [1/2], Step [13920/18063], Loss: 0.0002\n",
      "Epoch [1/2], Step [13950/18063], Loss: 0.0060\n",
      "Epoch [1/2], Step [13980/18063], Loss: 0.3104\n",
      "Epoch [1/2], Step [14010/18063], Loss: 0.0352\n",
      "Epoch [1/2], Step [14040/18063], Loss: 0.8341\n",
      "Epoch [1/2], Step [14070/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [14100/18063], Loss: 0.0002\n",
      "Epoch [1/2], Step [14130/18063], Loss: 0.2536\n",
      "Epoch [1/2], Step [14160/18063], Loss: 0.0008\n",
      "Epoch [1/2], Step [14190/18063], Loss: 1.2493\n",
      "Epoch [1/2], Step [14220/18063], Loss: 0.0015\n",
      "Epoch [1/2], Step [14250/18063], Loss: 1.4560\n",
      "Epoch [1/2], Step [14280/18063], Loss: 0.1824\n",
      "Epoch [1/2], Step [14310/18063], Loss: 3.8810\n",
      "Epoch [1/2], Step [14340/18063], Loss: 0.3725\n",
      "Epoch [1/2], Step [14370/18063], Loss: 0.0829\n",
      "Epoch [1/2], Step [14400/18063], Loss: 0.0483\n",
      "Epoch [1/2], Step [14430/18063], Loss: 0.0008\n",
      "Epoch [1/2], Step [14460/18063], Loss: 1.2110\n",
      "Epoch [1/2], Step [14490/18063], Loss: 0.0019\n",
      "Epoch [1/2], Step [14520/18063], Loss: 0.0011\n",
      "Epoch [1/2], Step [14550/18063], Loss: 0.4997\n",
      "Epoch [1/2], Step [14580/18063], Loss: 1.9039\n",
      "Epoch [1/2], Step [14610/18063], Loss: 0.1952\n",
      "Epoch [1/2], Step [14640/18063], Loss: 0.0157\n",
      "Epoch [1/2], Step [14670/18063], Loss: 1.0565\n",
      "Epoch [1/2], Step [14700/18063], Loss: 0.3019\n",
      "Epoch [1/2], Step [14730/18063], Loss: 0.0727\n",
      "Epoch [1/2], Step [14760/18063], Loss: 0.0025\n",
      "Epoch [1/2], Step [14790/18063], Loss: 0.2327\n",
      "Epoch [1/2], Step [14820/18063], Loss: 2.8395\n",
      "Epoch [1/2], Step [14850/18063], Loss: 0.2881\n",
      "Epoch [1/2], Step [14880/18063], Loss: 0.1842\n",
      "Epoch [1/2], Step [14910/18063], Loss: 0.0167\n",
      "Epoch [1/2], Step [14940/18063], Loss: 0.3216\n",
      "Epoch [1/2], Step [14970/18063], Loss: 2.3442\n",
      "Epoch [1/2], Step [15000/18063], Loss: 0.1866\n",
      "Epoch [1/2], Step [15030/18063], Loss: 0.1338\n",
      "Epoch [1/2], Step [15060/18063], Loss: 0.3604\n",
      "Epoch [1/2], Step [15090/18063], Loss: 0.1657\n",
      "Epoch [1/2], Step [15120/18063], Loss: 0.1698\n",
      "Epoch [1/2], Step [15150/18063], Loss: 0.0334\n",
      "Epoch [1/2], Step [15180/18063], Loss: 0.0006\n",
      "Epoch [1/2], Step [15210/18063], Loss: 0.1743\n",
      "Epoch [1/2], Step [15240/18063], Loss: 0.8690\n",
      "Epoch [1/2], Step [15270/18063], Loss: 0.0078\n",
      "Epoch [1/2], Step [15300/18063], Loss: 2.0846\n",
      "Epoch [1/2], Step [15330/18063], Loss: 0.1363\n",
      "Epoch [1/2], Step [15360/18063], Loss: 0.0243\n",
      "Epoch [1/2], Step [15390/18063], Loss: 0.0000\n",
      "Epoch [1/2], Step [15420/18063], Loss: 5.6816\n",
      "Epoch [1/2], Step [15450/18063], Loss: 0.0390\n",
      "Epoch [1/2], Step [15480/18063], Loss: 0.0007\n",
      "Epoch [1/2], Step [15510/18063], Loss: 3.5820\n",
      "Epoch [1/2], Step [15540/18063], Loss: 0.6736\n",
      "Epoch [1/2], Step [15570/18063], Loss: 0.3513\n",
      "Epoch [1/2], Step [15600/18063], Loss: 0.2399\n",
      "Epoch [1/2], Step [15630/18063], Loss: 0.4460\n",
      "Epoch [1/2], Step [15660/18063], Loss: 0.0026\n",
      "Epoch [1/2], Step [15690/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [15720/18063], Loss: 1.9264\n",
      "Epoch [1/2], Step [15750/18063], Loss: 3.6163\n",
      "Epoch [1/2], Step [15780/18063], Loss: 0.0009\n",
      "Epoch [1/2], Step [15810/18063], Loss: 0.3280\n",
      "Epoch [1/2], Step [15840/18063], Loss: 1.5717\n",
      "Epoch [1/2], Step [15870/18063], Loss: 0.2457\n",
      "Epoch [1/2], Step [15900/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [15930/18063], Loss: 0.0627\n",
      "Epoch [1/2], Step [15960/18063], Loss: 0.0043\n",
      "Epoch [1/2], Step [15990/18063], Loss: 0.0532\n",
      "Epoch [1/2], Step [16020/18063], Loss: 0.0015\n",
      "Epoch [1/2], Step [16050/18063], Loss: 0.0107\n",
      "Epoch [1/2], Step [16080/18063], Loss: 0.0029\n",
      "Epoch [1/2], Step [16110/18063], Loss: 0.0005\n",
      "Epoch [1/2], Step [16140/18063], Loss: 0.0003\n",
      "Epoch [1/2], Step [16170/18063], Loss: 0.0097\n",
      "Epoch [1/2], Step [16200/18063], Loss: 0.0005\n",
      "Epoch [1/2], Step [16230/18063], Loss: 0.0036\n",
      "Epoch [1/2], Step [16260/18063], Loss: 0.0010\n",
      "Epoch [1/2], Step [16290/18063], Loss: 0.0013\n",
      "Epoch [1/2], Step [16320/18063], Loss: 0.0021\n",
      "Epoch [1/2], Step [16350/18063], Loss: 0.1286\n",
      "Epoch [1/2], Step [16380/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [16410/18063], Loss: 0.0220\n",
      "Epoch [1/2], Step [16440/18063], Loss: 0.6593\n",
      "Epoch [1/2], Step [16470/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [16500/18063], Loss: 0.0007\n",
      "Epoch [1/2], Step [16530/18063], Loss: 1.1613\n",
      "Epoch [1/2], Step [16560/18063], Loss: 0.0027\n",
      "Epoch [1/2], Step [16590/18063], Loss: 1.0055\n",
      "Epoch [1/2], Step [16620/18063], Loss: 0.0013\n",
      "Epoch [1/2], Step [16650/18063], Loss: 1.7925\n",
      "Epoch [1/2], Step [16680/18063], Loss: 6.0223\n",
      "Epoch [1/2], Step [16710/18063], Loss: 0.0000\n",
      "Epoch [1/2], Step [16740/18063], Loss: 0.0017\n",
      "Epoch [1/2], Step [16770/18063], Loss: 0.0005\n",
      "Epoch [1/2], Step [16800/18063], Loss: 1.7955\n",
      "Epoch [1/2], Step [16830/18063], Loss: 0.8374\n",
      "Epoch [1/2], Step [16860/18063], Loss: 1.2428\n",
      "Epoch [1/2], Step [16890/18063], Loss: 0.1957\n",
      "Epoch [1/2], Step [16920/18063], Loss: 0.3823\n",
      "Epoch [1/2], Step [16950/18063], Loss: 1.9786\n",
      "Epoch [1/2], Step [16980/18063], Loss: 0.1346\n",
      "Epoch [1/2], Step [17010/18063], Loss: 0.9036\n",
      "Epoch [1/2], Step [17040/18063], Loss: 0.0165\n",
      "Epoch [1/2], Step [17070/18063], Loss: 1.5983\n",
      "Epoch [1/2], Step [17100/18063], Loss: 0.0000\n",
      "Epoch [1/2], Step [17130/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [17160/18063], Loss: 0.0001\n",
      "Epoch [1/2], Step [17190/18063], Loss: 0.0015\n",
      "Epoch [1/2], Step [17220/18063], Loss: 0.0000\n",
      "Epoch [1/2], Step [17250/18063], Loss: 0.0053\n",
      "Epoch [1/2], Step [17280/18063], Loss: 0.0043\n",
      "Epoch [1/2], Step [17310/18063], Loss: 0.0620\n",
      "Epoch [1/2], Step [17340/18063], Loss: 0.0236\n",
      "Epoch [1/2], Step [17370/18063], Loss: 1.3747\n",
      "Epoch [1/2], Step [17400/18063], Loss: 0.2179\n",
      "Epoch [1/2], Step [17430/18063], Loss: 0.1226\n",
      "Epoch [1/2], Step [17460/18063], Loss: 0.3504\n",
      "Epoch [1/2], Step [17490/18063], Loss: 0.4637\n",
      "Epoch [1/2], Step [17520/18063], Loss: 0.5366\n",
      "Epoch [1/2], Step [17550/18063], Loss: 0.5105\n",
      "Epoch [1/2], Step [17580/18063], Loss: 0.5081\n",
      "Epoch [1/2], Step [17610/18063], Loss: 0.6442\n",
      "Epoch [1/2], Step [17640/18063], Loss: 4.3713\n",
      "Epoch [1/2], Step [17670/18063], Loss: 0.0333\n",
      "Epoch [1/2], Step [17700/18063], Loss: 1.7165\n",
      "Epoch [1/2], Step [17730/18063], Loss: 0.0002\n",
      "Epoch [1/2], Step [17760/18063], Loss: 4.1521\n",
      "Epoch [1/2], Step [17790/18063], Loss: 0.0323\n",
      "Epoch [1/2], Step [17820/18063], Loss: 0.0046\n",
      "Epoch [1/2], Step [17850/18063], Loss: 0.7497\n",
      "Epoch [1/2], Step [17880/18063], Loss: 0.0309\n",
      "Epoch [1/2], Step [17910/18063], Loss: 0.0254\n",
      "Epoch [1/2], Step [17940/18063], Loss: 0.2986\n",
      "Epoch [1/2], Step [17970/18063], Loss: 5.8502\n",
      "Epoch [1/2], Step [18000/18063], Loss: 0.0083\n",
      "Epoch [1/2], Step [18030/18063], Loss: 0.1281\n",
      "Epoch [1/2], Step [18060/18063], Loss: 0.0136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7fa4aba01c461fa49978f6c44f1cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [30/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [60/18063], Loss: 0.4895\n",
      "Epoch [2/2], Step [90/18063], Loss: 0.0022\n",
      "Epoch [2/2], Step [120/18063], Loss: 0.1502\n",
      "Epoch [2/2], Step [150/18063], Loss: 0.0015\n",
      "Epoch [2/2], Step [180/18063], Loss: 4.0186\n",
      "Epoch [2/2], Step [210/18063], Loss: 0.9032\n",
      "Epoch [2/2], Step [240/18063], Loss: 1.7574\n",
      "Epoch [2/2], Step [270/18063], Loss: 0.0044\n",
      "Epoch [2/2], Step [300/18063], Loss: 0.1591\n",
      "Epoch [2/2], Step [330/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [360/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [390/18063], Loss: 1.5843\n",
      "Epoch [2/2], Step [420/18063], Loss: 0.0064\n",
      "Epoch [2/2], Step [450/18063], Loss: 0.0115\n",
      "Epoch [2/2], Step [480/18063], Loss: 0.1849\n",
      "Epoch [2/2], Step [510/18063], Loss: 0.0010\n",
      "Epoch [2/2], Step [540/18063], Loss: 0.1300\n",
      "Epoch [2/2], Step [570/18063], Loss: 4.7715\n",
      "Epoch [2/2], Step [600/18063], Loss: 0.0622\n",
      "Epoch [2/2], Step [630/18063], Loss: 1.6822\n",
      "Epoch [2/2], Step [660/18063], Loss: 0.0150\n",
      "Epoch [2/2], Step [690/18063], Loss: 0.0089\n",
      "Epoch [2/2], Step [720/18063], Loss: 0.0010\n",
      "Epoch [2/2], Step [750/18063], Loss: 0.0315\n",
      "Epoch [2/2], Step [780/18063], Loss: 1.4630\n",
      "Epoch [2/2], Step [810/18063], Loss: 0.0025\n",
      "Epoch [2/2], Step [840/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [870/18063], Loss: 0.9363\n",
      "Epoch [2/2], Step [900/18063], Loss: 0.0046\n",
      "Epoch [2/2], Step [930/18063], Loss: 0.0055\n",
      "Epoch [2/2], Step [960/18063], Loss: 0.9059\n",
      "Epoch [2/2], Step [990/18063], Loss: 0.0019\n",
      "Epoch [2/2], Step [1020/18063], Loss: 4.0341\n",
      "Epoch [2/2], Step [1050/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [1080/18063], Loss: 0.0332\n",
      "Epoch [2/2], Step [1110/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [1140/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [1170/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [1200/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [1230/18063], Loss: 0.6558\n",
      "Epoch [2/2], Step [1260/18063], Loss: 0.0040\n",
      "Epoch [2/2], Step [1290/18063], Loss: 0.9376\n",
      "Epoch [2/2], Step [1320/18063], Loss: 0.1101\n",
      "Epoch [2/2], Step [1350/18063], Loss: 0.0116\n",
      "Epoch [2/2], Step [1380/18063], Loss: 2.7568\n",
      "Epoch [2/2], Step [1410/18063], Loss: 0.0080\n",
      "Epoch [2/2], Step [1440/18063], Loss: 0.0009\n",
      "Epoch [2/2], Step [1470/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [1500/18063], Loss: 1.5921\n",
      "Epoch [2/2], Step [1530/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [1560/18063], Loss: 0.7016\n",
      "Epoch [2/2], Step [1590/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [1620/18063], Loss: 2.6650\n",
      "Epoch [2/2], Step [1650/18063], Loss: 0.7575\n",
      "Epoch [2/2], Step [1680/18063], Loss: 3.6071\n",
      "Epoch [2/2], Step [1710/18063], Loss: 0.0068\n",
      "Epoch [2/2], Step [1740/18063], Loss: 0.0857\n",
      "Epoch [2/2], Step [1770/18063], Loss: 1.0952\n",
      "Epoch [2/2], Step [1800/18063], Loss: 0.0345\n",
      "Epoch [2/2], Step [1830/18063], Loss: 0.0008\n",
      "Epoch [2/2], Step [1860/18063], Loss: 5.4739\n",
      "Epoch [2/2], Step [1890/18063], Loss: 0.0173\n",
      "Epoch [2/2], Step [1920/18063], Loss: 1.3544\n",
      "Epoch [2/2], Step [1950/18063], Loss: 0.0054\n",
      "Epoch [2/2], Step [1980/18063], Loss: 0.2346\n",
      "Epoch [2/2], Step [2010/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [2040/18063], Loss: 0.7617\n",
      "Epoch [2/2], Step [2070/18063], Loss: 0.7191\n",
      "Epoch [2/2], Step [2100/18063], Loss: 0.2784\n",
      "Epoch [2/2], Step [2130/18063], Loss: 0.2860\n",
      "Epoch [2/2], Step [2160/18063], Loss: 4.3836\n",
      "Epoch [2/2], Step [2190/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [2220/18063], Loss: 0.5843\n",
      "Epoch [2/2], Step [2250/18063], Loss: 2.5159\n",
      "Epoch [2/2], Step [2280/18063], Loss: 1.8564\n",
      "Epoch [2/2], Step [2310/18063], Loss: 2.9434\n",
      "Epoch [2/2], Step [2340/18063], Loss: 0.0386\n",
      "Epoch [2/2], Step [2370/18063], Loss: 0.0028\n",
      "Epoch [2/2], Step [2400/18063], Loss: 3.7947\n",
      "Epoch [2/2], Step [2430/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [2460/18063], Loss: 0.9277\n",
      "Epoch [2/2], Step [2490/18063], Loss: 0.0118\n",
      "Epoch [2/2], Step [2520/18063], Loss: 1.7931\n",
      "Epoch [2/2], Step [2550/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [2580/18063], Loss: 0.1602\n",
      "Epoch [2/2], Step [2610/18063], Loss: 0.0008\n",
      "Epoch [2/2], Step [2640/18063], Loss: 0.0048\n",
      "Epoch [2/2], Step [2670/18063], Loss: 1.2090\n",
      "Epoch [2/2], Step [2700/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [2730/18063], Loss: 0.0548\n",
      "Epoch [2/2], Step [2760/18063], Loss: 0.7566\n",
      "Epoch [2/2], Step [2790/18063], Loss: 2.9161\n",
      "Epoch [2/2], Step [2820/18063], Loss: 0.0070\n",
      "Epoch [2/2], Step [2850/18063], Loss: 0.0036\n",
      "Epoch [2/2], Step [2880/18063], Loss: 0.0059\n",
      "Epoch [2/2], Step [2910/18063], Loss: 0.0008\n",
      "Epoch [2/2], Step [2940/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [2970/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [3000/18063], Loss: 5.0389\n",
      "Epoch [2/2], Step [3030/18063], Loss: 3.3768\n",
      "Epoch [2/2], Step [3060/18063], Loss: 0.3941\n",
      "Epoch [2/2], Step [3090/18063], Loss: 0.0026\n",
      "Epoch [2/2], Step [3120/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [3150/18063], Loss: 0.0372\n",
      "Epoch [2/2], Step [3180/18063], Loss: 0.4160\n",
      "Epoch [2/2], Step [3210/18063], Loss: 0.0039\n",
      "Epoch [2/2], Step [3240/18063], Loss: 0.3592\n",
      "Epoch [2/2], Step [3270/18063], Loss: 0.0008\n",
      "Epoch [2/2], Step [3300/18063], Loss: 0.0059\n",
      "Epoch [2/2], Step [3330/18063], Loss: 0.5478\n",
      "Epoch [2/2], Step [3360/18063], Loss: 0.0115\n",
      "Epoch [2/2], Step [3390/18063], Loss: 3.0984\n",
      "Epoch [2/2], Step [3420/18063], Loss: 0.5382\n",
      "Epoch [2/2], Step [3450/18063], Loss: 1.2713\n",
      "Epoch [2/2], Step [3480/18063], Loss: 0.5240\n",
      "Epoch [2/2], Step [3510/18063], Loss: 0.0007\n",
      "Epoch [2/2], Step [3540/18063], Loss: 0.0036\n",
      "Epoch [2/2], Step [3570/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [3600/18063], Loss: 0.0039\n",
      "Epoch [2/2], Step [3630/18063], Loss: 0.0014\n",
      "Epoch [2/2], Step [3660/18063], Loss: 1.9267\n",
      "Epoch [2/2], Step [3690/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [3720/18063], Loss: 0.0021\n",
      "Epoch [2/2], Step [3750/18063], Loss: 0.0548\n",
      "Epoch [2/2], Step [3780/18063], Loss: 0.4502\n",
      "Epoch [2/2], Step [3810/18063], Loss: 0.0178\n",
      "Epoch [2/2], Step [3840/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [3870/18063], Loss: 0.7100\n",
      "Epoch [2/2], Step [3900/18063], Loss: 1.5676\n",
      "Epoch [2/2], Step [3930/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [3960/18063], Loss: 0.4101\n",
      "Epoch [2/2], Step [3990/18063], Loss: 0.0421\n",
      "Epoch [2/2], Step [4020/18063], Loss: 0.0014\n",
      "Epoch [2/2], Step [4050/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [4080/18063], Loss: 3.3218\n",
      "Epoch [2/2], Step [4110/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [4140/18063], Loss: 0.0444\n",
      "Epoch [2/2], Step [4170/18063], Loss: 0.0158\n",
      "Epoch [2/2], Step [4200/18063], Loss: 0.1604\n",
      "Epoch [2/2], Step [4230/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [4260/18063], Loss: 1.0452\n",
      "Epoch [2/2], Step [4290/18063], Loss: 0.0187\n",
      "Epoch [2/2], Step [4320/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [4350/18063], Loss: 4.2424\n",
      "Epoch [2/2], Step [4380/18063], Loss: 2.0792\n",
      "Epoch [2/2], Step [4410/18063], Loss: 0.3899\n",
      "Epoch [2/2], Step [4440/18063], Loss: 0.0181\n",
      "Epoch [2/2], Step [4470/18063], Loss: 0.0095\n",
      "Epoch [2/2], Step [4500/18063], Loss: 0.0120\n",
      "Epoch [2/2], Step [4530/18063], Loss: 0.0175\n",
      "Epoch [2/2], Step [4560/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [4590/18063], Loss: 0.0014\n",
      "Epoch [2/2], Step [4620/18063], Loss: 0.0033\n",
      "Epoch [2/2], Step [4650/18063], Loss: 1.8333\n",
      "Epoch [2/2], Step [4680/18063], Loss: 0.0335\n",
      "Epoch [2/2], Step [4710/18063], Loss: 0.0156\n",
      "Epoch [2/2], Step [4740/18063], Loss: 0.4581\n",
      "Epoch [2/2], Step [4770/18063], Loss: 0.0119\n",
      "Epoch [2/2], Step [4800/18063], Loss: 0.0028\n",
      "Epoch [2/2], Step [4830/18063], Loss: 0.0166\n",
      "Epoch [2/2], Step [4860/18063], Loss: 1.8655\n",
      "Epoch [2/2], Step [4890/18063], Loss: 3.5438\n",
      "Epoch [2/2], Step [4920/18063], Loss: 0.0063\n",
      "Epoch [2/2], Step [4950/18063], Loss: 0.0199\n",
      "Epoch [2/2], Step [4980/18063], Loss: 0.7493\n",
      "Epoch [2/2], Step [5010/18063], Loss: 0.0818\n",
      "Epoch [2/2], Step [5040/18063], Loss: 1.1407\n",
      "Epoch [2/2], Step [5070/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [5100/18063], Loss: 3.2650\n",
      "Epoch [2/2], Step [5130/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [5160/18063], Loss: 0.1191\n",
      "Epoch [2/2], Step [5190/18063], Loss: 1.2251\n",
      "Epoch [2/2], Step [5220/18063], Loss: 0.1539\n",
      "Epoch [2/2], Step [5250/18063], Loss: 0.2416\n",
      "Epoch [2/2], Step [5280/18063], Loss: 0.4319\n",
      "Epoch [2/2], Step [5310/18063], Loss: 0.0012\n",
      "Epoch [2/2], Step [5340/18063], Loss: 0.0379\n",
      "Epoch [2/2], Step [5370/18063], Loss: 0.0016\n",
      "Epoch [2/2], Step [5400/18063], Loss: 0.0529\n",
      "Epoch [2/2], Step [5430/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [5460/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [5490/18063], Loss: 0.3308\n",
      "Epoch [2/2], Step [5520/18063], Loss: 2.9053\n",
      "Epoch [2/2], Step [5550/18063], Loss: 1.4107\n",
      "Epoch [2/2], Step [5580/18063], Loss: 0.0034\n",
      "Epoch [2/2], Step [5610/18063], Loss: 0.0227\n",
      "Epoch [2/2], Step [5640/18063], Loss: 4.6467\n",
      "Epoch [2/2], Step [5670/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [5700/18063], Loss: 0.7918\n",
      "Epoch [2/2], Step [5730/18063], Loss: 0.0619\n",
      "Epoch [2/2], Step [5760/18063], Loss: 2.7028\n",
      "Epoch [2/2], Step [5790/18063], Loss: 1.0747\n",
      "Epoch [2/2], Step [5820/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [5850/18063], Loss: 0.2657\n",
      "Epoch [2/2], Step [5880/18063], Loss: 0.2909\n",
      "Epoch [2/2], Step [5910/18063], Loss: 0.9568\n",
      "Epoch [2/2], Step [5940/18063], Loss: 4.2406\n",
      "Epoch [2/2], Step [5970/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [6000/18063], Loss: 0.0314\n",
      "Epoch [2/2], Step [6030/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [6060/18063], Loss: 0.2057\n",
      "Epoch [2/2], Step [6090/18063], Loss: 0.0008\n",
      "Epoch [2/2], Step [6120/18063], Loss: 0.0007\n",
      "Epoch [2/2], Step [6150/18063], Loss: 0.0109\n",
      "Epoch [2/2], Step [6180/18063], Loss: 0.0014\n",
      "Epoch [2/2], Step [6210/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [6240/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [6270/18063], Loss: 0.1770\n",
      "Epoch [2/2], Step [6300/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [6330/18063], Loss: 0.0494\n",
      "Epoch [2/2], Step [6360/18063], Loss: 0.1239\n",
      "Epoch [2/2], Step [6390/18063], Loss: 0.0949\n",
      "Epoch [2/2], Step [6420/18063], Loss: 0.0012\n",
      "Epoch [2/2], Step [6450/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [6480/18063], Loss: 0.7181\n",
      "Epoch [2/2], Step [6510/18063], Loss: 0.2486\n",
      "Epoch [2/2], Step [6540/18063], Loss: 0.7477\n",
      "Epoch [2/2], Step [6570/18063], Loss: 0.0035\n",
      "Epoch [2/2], Step [6600/18063], Loss: 0.9672\n",
      "Epoch [2/2], Step [6630/18063], Loss: 0.0051\n",
      "Epoch [2/2], Step [6660/18063], Loss: 0.1162\n",
      "Epoch [2/2], Step [6690/18063], Loss: 0.8073\n",
      "Epoch [2/2], Step [6720/18063], Loss: 0.5947\n",
      "Epoch [2/2], Step [6750/18063], Loss: 0.0289\n",
      "Epoch [2/2], Step [6780/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [6810/18063], Loss: 0.0741\n",
      "Epoch [2/2], Step [6840/18063], Loss: 0.3209\n",
      "Epoch [2/2], Step [6870/18063], Loss: 0.0792\n",
      "Epoch [2/2], Step [6900/18063], Loss: 0.4537\n",
      "Epoch [2/2], Step [6930/18063], Loss: 0.7778\n",
      "Epoch [2/2], Step [6960/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [6990/18063], Loss: 0.0204\n",
      "Epoch [2/2], Step [7020/18063], Loss: 3.2849\n",
      "Epoch [2/2], Step [7050/18063], Loss: 0.0252\n",
      "Epoch [2/2], Step [7080/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [7110/18063], Loss: 0.6058\n",
      "Epoch [2/2], Step [7140/18063], Loss: 0.3649\n",
      "Epoch [2/2], Step [7170/18063], Loss: 0.0390\n",
      "Epoch [2/2], Step [7200/18063], Loss: 0.2062\n",
      "Epoch [2/2], Step [7230/18063], Loss: 0.4552\n",
      "Epoch [2/2], Step [7260/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [7290/18063], Loss: 2.1928\n",
      "Epoch [2/2], Step [7320/18063], Loss: 0.1553\n",
      "Epoch [2/2], Step [7350/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [7380/18063], Loss: 0.0503\n",
      "Epoch [2/2], Step [7410/18063], Loss: 0.0212\n",
      "Epoch [2/2], Step [7440/18063], Loss: 0.0257\n",
      "Epoch [2/2], Step [7470/18063], Loss: 0.0021\n",
      "Epoch [2/2], Step [7500/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [7530/18063], Loss: 0.1669\n",
      "Epoch [2/2], Step [7560/18063], Loss: 0.1788\n",
      "Epoch [2/2], Step [7590/18063], Loss: 0.3017\n",
      "Epoch [2/2], Step [7620/18063], Loss: 0.0012\n",
      "Epoch [2/2], Step [7650/18063], Loss: 0.0816\n",
      "Epoch [2/2], Step [7680/18063], Loss: 2.3380\n",
      "Epoch [2/2], Step [7710/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [7740/18063], Loss: 1.8346\n",
      "Epoch [2/2], Step [7770/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [7800/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [7830/18063], Loss: 0.0012\n",
      "Epoch [2/2], Step [7860/18063], Loss: 0.0009\n",
      "Epoch [2/2], Step [7890/18063], Loss: 0.1973\n",
      "Epoch [2/2], Step [7920/18063], Loss: 0.0764\n",
      "Epoch [2/2], Step [7950/18063], Loss: 0.0340\n",
      "Epoch [2/2], Step [7980/18063], Loss: 0.0011\n",
      "Epoch [2/2], Step [8010/18063], Loss: 0.0020\n",
      "Epoch [2/2], Step [8040/18063], Loss: 0.2040\n",
      "Epoch [2/2], Step [8070/18063], Loss: 0.9976\n",
      "Epoch [2/2], Step [8100/18063], Loss: 0.0015\n",
      "Epoch [2/2], Step [8130/18063], Loss: 0.0329\n",
      "Epoch [2/2], Step [8160/18063], Loss: 2.6244\n",
      "Epoch [2/2], Step [8190/18063], Loss: 0.0010\n",
      "Epoch [2/2], Step [8220/18063], Loss: 0.0021\n",
      "Epoch [2/2], Step [8250/18063], Loss: 0.4967\n",
      "Epoch [2/2], Step [8280/18063], Loss: 0.4373\n",
      "Epoch [2/2], Step [8310/18063], Loss: 0.9636\n",
      "Epoch [2/2], Step [8340/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [8370/18063], Loss: 0.0232\n",
      "Epoch [2/2], Step [8400/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [8430/18063], Loss: 0.2842\n",
      "Epoch [2/2], Step [8460/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [8490/18063], Loss: 0.0298\n",
      "Epoch [2/2], Step [8520/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [8550/18063], Loss: 0.0126\n",
      "Epoch [2/2], Step [8580/18063], Loss: 0.0235\n",
      "Epoch [2/2], Step [8610/18063], Loss: 1.5616\n",
      "Epoch [2/2], Step [8640/18063], Loss: 0.0613\n",
      "Epoch [2/2], Step [8670/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [8700/18063], Loss: 0.0086\n",
      "Epoch [2/2], Step [8730/18063], Loss: 2.6160\n",
      "Epoch [2/2], Step [8760/18063], Loss: 0.0533\n",
      "Epoch [2/2], Step [8790/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [8820/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [8850/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [8880/18063], Loss: 0.0045\n",
      "Epoch [2/2], Step [8910/18063], Loss: 0.3958\n",
      "Epoch [2/2], Step [8940/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [8970/18063], Loss: 0.1925\n",
      "Epoch [2/2], Step [9000/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [9030/18063], Loss: 0.0094\n",
      "Epoch [2/2], Step [9060/18063], Loss: 0.9703\n",
      "Epoch [2/2], Step [9090/18063], Loss: 1.2676\n",
      "Epoch [2/2], Step [9120/18063], Loss: 0.0037\n",
      "Epoch [2/2], Step [9150/18063], Loss: 0.0019\n",
      "Epoch [2/2], Step [9180/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [9210/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [9240/18063], Loss: 0.0236\n",
      "Epoch [2/2], Step [9270/18063], Loss: 0.8543\n",
      "Epoch [2/2], Step [9300/18063], Loss: 0.5965\n",
      "Epoch [2/2], Step [9330/18063], Loss: 0.8125\n",
      "Epoch [2/2], Step [9360/18063], Loss: 0.0407\n",
      "Epoch [2/2], Step [9390/18063], Loss: 1.8554\n",
      "Epoch [2/2], Step [9420/18063], Loss: 0.2821\n",
      "Epoch [2/2], Step [9450/18063], Loss: 0.0570\n",
      "Epoch [2/2], Step [9480/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [9510/18063], Loss: 0.0879\n",
      "Epoch [2/2], Step [9540/18063], Loss: 0.2493\n",
      "Epoch [2/2], Step [9570/18063], Loss: 3.7293\n",
      "Epoch [2/2], Step [9600/18063], Loss: 1.1340\n",
      "Epoch [2/2], Step [9630/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [9660/18063], Loss: 0.0945\n",
      "Epoch [2/2], Step [9690/18063], Loss: 0.0383\n",
      "Epoch [2/2], Step [9720/18063], Loss: 0.7755\n",
      "Epoch [2/2], Step [9750/18063], Loss: 0.0017\n",
      "Epoch [2/2], Step [9780/18063], Loss: 0.0768\n",
      "Epoch [2/2], Step [9810/18063], Loss: 0.0874\n",
      "Epoch [2/2], Step [9840/18063], Loss: 4.5182\n",
      "Epoch [2/2], Step [9870/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [9900/18063], Loss: 0.0121\n",
      "Epoch [2/2], Step [9930/18063], Loss: 2.7364\n",
      "Epoch [2/2], Step [9960/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [9990/18063], Loss: 0.0896\n",
      "Epoch [2/2], Step [10020/18063], Loss: 0.0053\n",
      "Epoch [2/2], Step [10050/18063], Loss: 0.6303\n",
      "Epoch [2/2], Step [10080/18063], Loss: 5.6605\n",
      "Epoch [2/2], Step [10110/18063], Loss: 1.9708\n",
      "Epoch [2/2], Step [10140/18063], Loss: 0.5272\n",
      "Epoch [2/2], Step [10170/18063], Loss: 0.7083\n",
      "Epoch [2/2], Step [10200/18063], Loss: 0.0048\n",
      "Epoch [2/2], Step [10230/18063], Loss: 0.0038\n",
      "Epoch [2/2], Step [10260/18063], Loss: 0.4489\n",
      "Epoch [2/2], Step [10290/18063], Loss: 0.0686\n",
      "Epoch [2/2], Step [10320/18063], Loss: 0.0100\n",
      "Epoch [2/2], Step [10350/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [10380/18063], Loss: 0.3862\n",
      "Epoch [2/2], Step [10410/18063], Loss: 0.0072\n",
      "Epoch [2/2], Step [10440/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [10470/18063], Loss: 0.2140\n",
      "Epoch [2/2], Step [10500/18063], Loss: 0.6698\n",
      "Epoch [2/2], Step [10530/18063], Loss: 0.0015\n",
      "Epoch [2/2], Step [10560/18063], Loss: 0.1352\n",
      "Epoch [2/2], Step [10590/18063], Loss: 0.0403\n",
      "Epoch [2/2], Step [10620/18063], Loss: 0.1026\n",
      "Epoch [2/2], Step [10650/18063], Loss: 0.0211\n",
      "Epoch [2/2], Step [10680/18063], Loss: 0.0321\n",
      "Epoch [2/2], Step [10710/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [10740/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [10770/18063], Loss: 0.0058\n",
      "Epoch [2/2], Step [10800/18063], Loss: 0.1099\n",
      "Epoch [2/2], Step [10830/18063], Loss: 0.0269\n",
      "Epoch [2/2], Step [10860/18063], Loss: 0.5848\n",
      "Epoch [2/2], Step [10890/18063], Loss: 0.0017\n",
      "Epoch [2/2], Step [10920/18063], Loss: 0.7613\n",
      "Epoch [2/2], Step [10950/18063], Loss: 0.1697\n",
      "Epoch [2/2], Step [10980/18063], Loss: 0.0016\n",
      "Epoch [2/2], Step [11010/18063], Loss: 0.1535\n",
      "Epoch [2/2], Step [11040/18063], Loss: 0.1108\n",
      "Epoch [2/2], Step [11070/18063], Loss: 0.0262\n",
      "Epoch [2/2], Step [11100/18063], Loss: 0.0998\n",
      "Epoch [2/2], Step [11130/18063], Loss: 0.0058\n",
      "Epoch [2/2], Step [11160/18063], Loss: 0.5359\n",
      "Epoch [2/2], Step [11190/18063], Loss: 2.4500\n",
      "Epoch [2/2], Step [11220/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [11250/18063], Loss: 3.9437\n",
      "Epoch [2/2], Step [11280/18063], Loss: 0.0790\n",
      "Epoch [2/2], Step [11310/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [11340/18063], Loss: 0.0028\n",
      "Epoch [2/2], Step [11370/18063], Loss: 0.0030\n",
      "Epoch [2/2], Step [11400/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [11430/18063], Loss: 0.0437\n",
      "Epoch [2/2], Step [11460/18063], Loss: 1.8431\n",
      "Epoch [2/2], Step [11490/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [11520/18063], Loss: 0.0026\n",
      "Epoch [2/2], Step [11550/18063], Loss: 1.2464\n",
      "Epoch [2/2], Step [11580/18063], Loss: 0.0047\n",
      "Epoch [2/2], Step [11610/18063], Loss: 0.0020\n",
      "Epoch [2/2], Step [11640/18063], Loss: 0.0299\n",
      "Epoch [2/2], Step [11670/18063], Loss: 3.7986\n",
      "Epoch [2/2], Step [11700/18063], Loss: 0.0089\n",
      "Epoch [2/2], Step [11730/18063], Loss: 4.8043\n",
      "Epoch [2/2], Step [11760/18063], Loss: 0.2570\n",
      "Epoch [2/2], Step [11790/18063], Loss: 3.6532\n",
      "Epoch [2/2], Step [11820/18063], Loss: 0.0083\n",
      "Epoch [2/2], Step [11850/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [11880/18063], Loss: 0.0025\n",
      "Epoch [2/2], Step [11910/18063], Loss: 0.0113\n",
      "Epoch [2/2], Step [11940/18063], Loss: 1.4208\n",
      "Epoch [2/2], Step [11970/18063], Loss: 1.9420\n",
      "Epoch [2/2], Step [12000/18063], Loss: 0.0167\n",
      "Epoch [2/2], Step [12030/18063], Loss: 1.0095\n",
      "Epoch [2/2], Step [12060/18063], Loss: 0.4249\n",
      "Epoch [2/2], Step [12090/18063], Loss: 0.2179\n",
      "Epoch [2/2], Step [12120/18063], Loss: 0.3871\n",
      "Epoch [2/2], Step [12150/18063], Loss: 0.0792\n",
      "Epoch [2/2], Step [12180/18063], Loss: 0.8239\n",
      "Epoch [2/2], Step [12210/18063], Loss: 0.0075\n",
      "Epoch [2/2], Step [12240/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [12270/18063], Loss: 3.8390\n",
      "Epoch [2/2], Step [12300/18063], Loss: 0.8303\n",
      "Epoch [2/2], Step [12330/18063], Loss: 0.1006\n",
      "Epoch [2/2], Step [12360/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [12390/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [12420/18063], Loss: 1.2703\n",
      "Epoch [2/2], Step [12450/18063], Loss: 1.2562\n",
      "Epoch [2/2], Step [12480/18063], Loss: 0.0034\n",
      "Epoch [2/2], Step [12510/18063], Loss: 0.5825\n",
      "Epoch [2/2], Step [12540/18063], Loss: 2.3268\n",
      "Epoch [2/2], Step [12570/18063], Loss: 1.0480\n",
      "Epoch [2/2], Step [12600/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [12630/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [12660/18063], Loss: 0.7428\n",
      "Epoch [2/2], Step [12690/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [12720/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [12750/18063], Loss: 4.1733\n",
      "Epoch [2/2], Step [12780/18063], Loss: 0.5592\n",
      "Epoch [2/2], Step [12810/18063], Loss: 0.4324\n",
      "Epoch [2/2], Step [12840/18063], Loss: 0.0028\n",
      "Epoch [2/2], Step [12870/18063], Loss: 0.5417\n",
      "Epoch [2/2], Step [12900/18063], Loss: 0.0009\n",
      "Epoch [2/2], Step [12930/18063], Loss: 0.0021\n",
      "Epoch [2/2], Step [12960/18063], Loss: 0.0055\n",
      "Epoch [2/2], Step [12990/18063], Loss: 0.7029\n",
      "Epoch [2/2], Step [13020/18063], Loss: 1.0442\n",
      "Epoch [2/2], Step [13050/18063], Loss: 2.2881\n",
      "Epoch [2/2], Step [13080/18063], Loss: 4.0114\n",
      "Epoch [2/2], Step [13110/18063], Loss: 3.1077\n",
      "Epoch [2/2], Step [13140/18063], Loss: 6.6314\n",
      "Epoch [2/2], Step [13170/18063], Loss: 0.1807\n",
      "Epoch [2/2], Step [13200/18063], Loss: 0.3892\n",
      "Epoch [2/2], Step [13230/18063], Loss: 0.0598\n",
      "Epoch [2/2], Step [13260/18063], Loss: 0.0042\n",
      "Epoch [2/2], Step [13290/18063], Loss: 0.0931\n",
      "Epoch [2/2], Step [13320/18063], Loss: 0.0991\n",
      "Epoch [2/2], Step [13350/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [13380/18063], Loss: 0.0469\n",
      "Epoch [2/2], Step [13410/18063], Loss: 0.1938\n",
      "Epoch [2/2], Step [13440/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [13470/18063], Loss: 2.8357\n",
      "Epoch [2/2], Step [13500/18063], Loss: 0.7264\n",
      "Epoch [2/2], Step [13530/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [13560/18063], Loss: 0.5890\n",
      "Epoch [2/2], Step [13590/18063], Loss: 1.2705\n",
      "Epoch [2/2], Step [13620/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [13650/18063], Loss: 0.1898\n",
      "Epoch [2/2], Step [13680/18063], Loss: 0.0019\n",
      "Epoch [2/2], Step [13710/18063], Loss: 0.0012\n",
      "Epoch [2/2], Step [13740/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [13770/18063], Loss: 0.4503\n",
      "Epoch [2/2], Step [13800/18063], Loss: 0.3107\n",
      "Epoch [2/2], Step [13830/18063], Loss: 0.0407\n",
      "Epoch [2/2], Step [13860/18063], Loss: 0.0220\n",
      "Epoch [2/2], Step [13890/18063], Loss: 0.8411\n",
      "Epoch [2/2], Step [13920/18063], Loss: 0.0077\n",
      "Epoch [2/2], Step [13950/18063], Loss: 0.8908\n",
      "Epoch [2/2], Step [13980/18063], Loss: 0.2260\n",
      "Epoch [2/2], Step [14010/18063], Loss: 2.0142\n",
      "Epoch [2/2], Step [14040/18063], Loss: 0.5446\n",
      "Epoch [2/2], Step [14070/18063], Loss: 0.0278\n",
      "Epoch [2/2], Step [14100/18063], Loss: 0.0268\n",
      "Epoch [2/2], Step [14130/18063], Loss: 0.1595\n",
      "Epoch [2/2], Step [14160/18063], Loss: 0.0194\n",
      "Epoch [2/2], Step [14190/18063], Loss: 0.0078\n",
      "Epoch [2/2], Step [14220/18063], Loss: 0.0105\n",
      "Epoch [2/2], Step [14250/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [14280/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [14310/18063], Loss: 4.1042\n",
      "Epoch [2/2], Step [14340/18063], Loss: 0.0009\n",
      "Epoch [2/2], Step [14370/18063], Loss: 0.0032\n",
      "Epoch [2/2], Step [14400/18063], Loss: 0.0023\n",
      "Epoch [2/2], Step [14430/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [14460/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [14490/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [14520/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [14550/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [14580/18063], Loss: 0.1639\n",
      "Epoch [2/2], Step [14610/18063], Loss: 0.0007\n",
      "Epoch [2/2], Step [14640/18063], Loss: 0.0509\n",
      "Epoch [2/2], Step [14670/18063], Loss: 0.7339\n",
      "Epoch [2/2], Step [14700/18063], Loss: 0.0017\n",
      "Epoch [2/2], Step [14730/18063], Loss: 0.0007\n",
      "Epoch [2/2], Step [14760/18063], Loss: 1.7196\n",
      "Epoch [2/2], Step [14790/18063], Loss: 0.0862\n",
      "Epoch [2/2], Step [14820/18063], Loss: 0.0367\n",
      "Epoch [2/2], Step [14850/18063], Loss: 1.1546\n",
      "Epoch [2/2], Step [14880/18063], Loss: 0.0015\n",
      "Epoch [2/2], Step [14910/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [14940/18063], Loss: 0.0039\n",
      "Epoch [2/2], Step [14970/18063], Loss: 1.4216\n",
      "Epoch [2/2], Step [15000/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [15030/18063], Loss: 0.0159\n",
      "Epoch [2/2], Step [15060/18063], Loss: 1.5788\n",
      "Epoch [2/2], Step [15090/18063], Loss: 0.0036\n",
      "Epoch [2/2], Step [15120/18063], Loss: 2.7164\n",
      "Epoch [2/2], Step [15150/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [15180/18063], Loss: 0.0016\n",
      "Epoch [2/2], Step [15210/18063], Loss: 0.0690\n",
      "Epoch [2/2], Step [15240/18063], Loss: 1.1822\n",
      "Epoch [2/2], Step [15270/18063], Loss: 3.1870\n",
      "Epoch [2/2], Step [15300/18063], Loss: 0.0028\n",
      "Epoch [2/2], Step [15330/18063], Loss: 1.6128\n",
      "Epoch [2/2], Step [15360/18063], Loss: 0.1078\n",
      "Epoch [2/2], Step [15390/18063], Loss: 0.0070\n",
      "Epoch [2/2], Step [15420/18063], Loss: 0.0026\n",
      "Epoch [2/2], Step [15450/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [15480/18063], Loss: 2.1982\n",
      "Epoch [2/2], Step [15510/18063], Loss: 1.5455\n",
      "Epoch [2/2], Step [15540/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [15570/18063], Loss: 1.3477\n",
      "Epoch [2/2], Step [15600/18063], Loss: 0.5036\n",
      "Epoch [2/2], Step [15630/18063], Loss: 0.0001\n",
      "Epoch [2/2], Step [15660/18063], Loss: 0.1072\n",
      "Epoch [2/2], Step [15690/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [15720/18063], Loss: 0.0145\n",
      "Epoch [2/2], Step [15750/18063], Loss: 5.7587\n",
      "Epoch [2/2], Step [15780/18063], Loss: 0.2735\n",
      "Epoch [2/2], Step [15810/18063], Loss: 0.0561\n",
      "Epoch [2/2], Step [15840/18063], Loss: 2.7177\n",
      "Epoch [2/2], Step [15870/18063], Loss: 0.4438\n",
      "Epoch [2/2], Step [15900/18063], Loss: 0.0073\n",
      "Epoch [2/2], Step [15930/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [15960/18063], Loss: 0.4166\n",
      "Epoch [2/2], Step [15990/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [16020/18063], Loss: 0.4464\n",
      "Epoch [2/2], Step [16050/18063], Loss: 0.8155\n",
      "Epoch [2/2], Step [16080/18063], Loss: 0.1750\n",
      "Epoch [2/2], Step [16110/18063], Loss: 0.9651\n",
      "Epoch [2/2], Step [16140/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [16170/18063], Loss: 0.0025\n",
      "Epoch [2/2], Step [16200/18063], Loss: 0.0014\n",
      "Epoch [2/2], Step [16230/18063], Loss: 0.0035\n",
      "Epoch [2/2], Step [16260/18063], Loss: 0.7958\n",
      "Epoch [2/2], Step [16290/18063], Loss: 0.0011\n",
      "Epoch [2/2], Step [16320/18063], Loss: 0.0208\n",
      "Epoch [2/2], Step [16350/18063], Loss: 0.0192\n",
      "Epoch [2/2], Step [16380/18063], Loss: 0.1223\n",
      "Epoch [2/2], Step [16410/18063], Loss: 0.0152\n",
      "Epoch [2/2], Step [16440/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [16470/18063], Loss: 0.0068\n",
      "Epoch [2/2], Step [16500/18063], Loss: 0.0013\n",
      "Epoch [2/2], Step [16530/18063], Loss: 3.1976\n",
      "Epoch [2/2], Step [16560/18063], Loss: 0.0038\n",
      "Epoch [2/2], Step [16590/18063], Loss: 0.0097\n",
      "Epoch [2/2], Step [16620/18063], Loss: 0.2574\n",
      "Epoch [2/2], Step [16650/18063], Loss: 0.0007\n",
      "Epoch [2/2], Step [16680/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [16710/18063], Loss: 0.0000\n",
      "Epoch [2/2], Step [16740/18063], Loss: 0.0007\n",
      "Epoch [2/2], Step [16770/18063], Loss: 5.8360\n",
      "Epoch [2/2], Step [16800/18063], Loss: 0.0005\n",
      "Epoch [2/2], Step [16830/18063], Loss: 0.0918\n",
      "Epoch [2/2], Step [16860/18063], Loss: 0.0630\n",
      "Epoch [2/2], Step [16890/18063], Loss: 1.8509\n",
      "Epoch [2/2], Step [16920/18063], Loss: 0.0006\n",
      "Epoch [2/2], Step [16950/18063], Loss: 0.0034\n",
      "Epoch [2/2], Step [16980/18063], Loss: 0.1835\n",
      "Epoch [2/2], Step [17010/18063], Loss: 0.0053\n",
      "Epoch [2/2], Step [17040/18063], Loss: 0.2251\n",
      "Epoch [2/2], Step [17070/18063], Loss: 0.4778\n",
      "Epoch [2/2], Step [17100/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [17130/18063], Loss: 0.4437\n",
      "Epoch [2/2], Step [17160/18063], Loss: 0.8636\n",
      "Epoch [2/2], Step [17190/18063], Loss: 0.0040\n",
      "Epoch [2/2], Step [17220/18063], Loss: 0.0479\n",
      "Epoch [2/2], Step [17250/18063], Loss: 0.1358\n",
      "Epoch [2/2], Step [17280/18063], Loss: 0.6535\n",
      "Epoch [2/2], Step [17310/18063], Loss: 2.4263\n",
      "Epoch [2/2], Step [17340/18063], Loss: 0.0074\n",
      "Epoch [2/2], Step [17370/18063], Loss: 0.5915\n",
      "Epoch [2/2], Step [17400/18063], Loss: 2.0878\n",
      "Epoch [2/2], Step [17430/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [17460/18063], Loss: 0.0215\n",
      "Epoch [2/2], Step [17490/18063], Loss: 0.6727\n",
      "Epoch [2/2], Step [17520/18063], Loss: 0.0010\n",
      "Epoch [2/2], Step [17550/18063], Loss: 0.0865\n",
      "Epoch [2/2], Step [17580/18063], Loss: 0.4062\n",
      "Epoch [2/2], Step [17610/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [17640/18063], Loss: 0.7595\n",
      "Epoch [2/2], Step [17670/18063], Loss: 0.0012\n",
      "Epoch [2/2], Step [17700/18063], Loss: 3.9762\n",
      "Epoch [2/2], Step [17730/18063], Loss: 0.1179\n",
      "Epoch [2/2], Step [17760/18063], Loss: 0.0168\n",
      "Epoch [2/2], Step [17790/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [17820/18063], Loss: 0.0034\n",
      "Epoch [2/2], Step [17850/18063], Loss: 0.0130\n",
      "Epoch [2/2], Step [17880/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [17910/18063], Loss: 0.0985\n",
      "Epoch [2/2], Step [17940/18063], Loss: 3.5589\n",
      "Epoch [2/2], Step [17970/18063], Loss: 0.0003\n",
      "Epoch [2/2], Step [18000/18063], Loss: 1.1870\n",
      "Epoch [2/2], Step [18030/18063], Loss: 0.7119\n",
      "Epoch [2/2], Step [18060/18063], Loss: 2.4394\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(tqdm(train_dataset)):\n",
    "        output = model(name.to(device))\n",
    "        loss = criterion(output, label.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \" f\"Step [{i + 1}/{len(train_dataset)}], \" f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.5147%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        output = model(name.to(device))\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred.cpu() == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese\n",
      "German\n",
      "Chinese\n",
      "Chinese\n",
      "Italian\n",
      "Russian\n"
     ]
    }
   ],
   "source": [
    "def pytorch_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_name.to(device))\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()\n",
    "    return label2lang[pred.item()]\n",
    "\n",
    "\n",
    "for name in ['Mike', 'Michael', 'Qin', 'Zhang', 'Slaveya', 'Svanovski']:\n",
    "    print(pytorch_predict(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "9aaf395b-f48d-48d4-8144-9e21ec14eeb7",
  "filePath": "/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/pytorch_rnn.ipynb",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

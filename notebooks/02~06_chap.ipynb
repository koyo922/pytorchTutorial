{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.6079e-09])\n",
      "tensor([9.9390e-16, 4.5740e-41, 8.2935e-35])\n",
      "tensor([[8.2916e-35, 0.0000e+00, 8.3169e-38],\n",
      "        [0.0000e+00, 1.1495e+24, 3.0881e+29]])\n",
      "tensor([[[9.9391e-16, 4.5740e-41, 8.2994e-35],\n",
      "         [0.0000e+00, 1.4013e-45, 0.0000e+00]],\n",
      "\n",
      "        [[7.5189e-35, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
      "tensor([[0.3488, 0.8176, 0.2171],\n",
      "        [0.5176, 0.3927, 0.7726],\n",
      "        [0.9623, 0.6454, 0.8372],\n",
      "        [0.1860, 0.5485, 0.7239],\n",
      "        [0.9249, 0.9518, 0.8578]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Everything in pytorch is based on Tensor operations.\n",
    "# A tensor can have different dimensions\n",
    "# so it can be 1d, 2d, or even 3d and higher\n",
    "\n",
    "# scalar, vector, matrix, tensor\n",
    "\n",
    "# torch.empty(size): uninitiallized\n",
    "x = torch.empty(1) # scalar\n",
    "print(x)\n",
    "x = torch.empty(3) # vector, 1D\n",
    "print(x)\n",
    "x = torch.empty(2,3) # matrix, 2D\n",
    "print(x)\n",
    "x = torch.empty(2,2,3) # tensor, 3 dimensions\n",
    "#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
    "print(x)\n",
    "\n",
    "# torch.rand(size): random numbers [0, 1]\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "# torch.zeros(size), fill with 0\n",
    "# torch.ones(size), fill with 1\n",
    "x = torch.zeros(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float16)\n",
      "torch.float16\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(x.size())\n",
    "\n",
    "# check data type\n",
    "print(x.dtype)\n",
    "\n",
    "# specify types, float32 default\n",
    "x = torch.zeros(5, 3, dtype=torch.float16)\n",
    "print(x)\n",
    "\n",
    "# check type\n",
    "print(x.dtype)\n",
    "\n",
    "# construct from data\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad argument\n",
    "# This will tell pytorch that it will need to calculate the gradients for this tensor\n",
    "# later in your optimization steps\n",
    "# i.e. this is a variable in your model that you want to optimize\n",
    "x = torch.tensor([5.5, 3], requires_grad=True)\n",
    "\n",
    "# Operations\n",
    "y = torch.rand(2, 2)\n",
    "x = torch.rand(2, 2)\n",
    "\n",
    "# elementwise addition\n",
    "z = x + y\n",
    "# torch.add(x,y)\n",
    "\n",
    "# in place addition, everythin with a trailing underscore is an inplace operation\n",
    "# i.e. it will modify the variable\n",
    "# y.add_(x)\n",
    "\n",
    "# substraction\n",
    "z = x - y\n",
    "z = torch.sub(x, y)\n",
    "\n",
    "# multiplication\n",
    "z = x * y\n",
    "z = torch.mul(x,y)\n",
    "\n",
    "# division\n",
    "z = x / y\n",
    "z = torch.div(x,y)\n",
    "\n",
    "# Slicing\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:, 0]) # all rows, column 0\n",
    "print(x[1, :]) # row 1, all columns\n",
    "print(x[1,1]) # element at 1, 1\n",
    "\n",
    "# Get the actual value if only 1 element in your tensor\n",
    "print(x[1,1].item())\n",
    "\n",
    "# Reshape with torch.view()\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "# if -1 it pytorch will automatically determine the necessary size\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "# Numpy\n",
    "# Converting a Torch Tensor to a NumPy array and vice versa is very easy\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "# torch to numpy with .numpy()\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n",
    "\n",
    "# Carful: If the Tensor is on the CPU (not the GPU),\n",
    "# both objects will share the same memory location, so changing one\n",
    "# will also change the other\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# numpy to torch with .from_numpy(x)\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# again be careful when modifying\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# by default all tensors are created on the CPU,\n",
    "# but you can also move them to the GPU (only if it's available )\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    # z = z.numpy() # not possible because numpy cannot handle GPU tenors\n",
    "    # move to CPU again\n",
    "    z.to(\"cpu\")       # ``.to`` can also change dtype together!\n",
    "    # z = z.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.device('cuda')\n",
    "\n",
    "a = torch.ones(5, device=cuda)\n",
    "b = a.cpu().numpy()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "a += 0.5\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1.5000, 1.5000, 1.5000, 1.5000, 1.5000])\n",
      "[1.5 1.5 1.5 1.5 1.5]\n"
     ]
    }
   ],
   "source": [
    "a2 = torch.ones(5, device='cpu')\n",
    "b = a2.numpy()\n",
    "\n",
    "print(a2)\n",
    "print(b)\n",
    "a2 += 0.5\n",
    "print(a2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2731,  0.2370, -1.2228], requires_grad=True)\n",
      "tensor([2.2731, 2.2370, 0.7772], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f7fd2af6c40>\n",
      "tensor([15.5011, 15.0124,  1.8120], grad_fn=<MulBackward0>)\n",
      "tensor(10.7752, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# The autograd package provides automatic differentiation \n",
    "# for all operations on Tensors\n",
    "\n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求导数, 注意 `z=z.mean()` 会初以size\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\because z &= 3 \\cdot y^2 / 3 \\\\\n",
    "  &= (x+2)^2 \\\\\n",
    "  &= x^2+4x+4 \\\\\n",
    "  \\\\\n",
    "\\therefore \\frac{\\partial z}{ \\partial x} &= 2x + 4\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5462, 4.4740, 1.5543])\n",
      "tensor([4.5462, 4.4740, 1.5543], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "print(2*x + 4) # dz/dx\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求导数\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\because\n",
    "\\vec{y} &= (\\vec{x} * 2) * 2^{10} \\\\\n",
    "  &= \\vec{x} * 2^{11} \\\\\n",
    "\\therefore \\frac{\\partial \\vec{y}}{ \\partial \\vec{x}}\n",
    "  &= diag(2^{11})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2179.4465,  -311.4295,  2190.5410], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------\n",
    "# Model with non-scalar output:\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "# needed for vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)\n",
    "\n",
    "print(v * 2**11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x7f7fd2a35ee0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------\n",
    "# Stop a tensor from tracking history:\n",
    "# For example during our training loop when we want to update our weights\n",
    "# then this update operation should not be part of the gradient computation\n",
    "# - x.requires_grad_(False)\n",
    "# - x.detach()\n",
    "# - wrap in 'with torch.no_grad():'\n",
    "\n",
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(b.grad_fn)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "b = a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "tensor(4.8000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------\n",
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training:\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 首轮\n",
    "weights = [1,1,1,1]\n",
    "output = (3*x1 + 3*x2 + 3*x3 + 3*x4)\n",
    "weights_grad = [3,3,3,3]\n",
    "weights = [1,1,1,1] - [3,3,3,3]*0.1 = [0.7,0.7,0.7,0.7]\n",
    "\n",
    "# 第二轮时\n",
    "weights_grad = [3,3,3,3] # 仍然为常数\n",
    "weights = weights - [3,3,3,3]*0.1 = [0.4,0.4,0.4,0.4]\n",
    "\n",
    "# 第三轮时\n",
    "output = (3*weights).sum() = 3*0.4*4 =4.8\n",
    "weights = weights - [3,3,3,3]*0.1 = [0.1,0.1,0.1,0.1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap 04\n",
    "ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #0: w=0.3000, l=30.0000\n",
      "iter #10: w=1.6653, l=1.1628\n",
      "iter #20: w=1.9341, l=0.0451\n",
      "iter #30: w=1.9870, l=0.0017\n",
      "iter #40: w=1.9974, l=0.0001\n",
      "iter #50: w=1.9995, l=0.0000\n",
      "iter #60: w=1.9999, l=0.0000\n",
      "iter #70: w=2.0000, l=0.0000\n",
      "iter #80: w=2.0000, l=0.0000\n",
      "iter #90: w=2.0000, l=0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32, device=device)\n",
    "Y = X * 2  # 注意: 这里举例不太好，是向量x使用标量w映射到向量y\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "lr = 1e-2\n",
    "n_iter = 100\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(Y, y_pred)  # 注意别写反顺序了\n",
    "    l.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "    w.grad.zero_()  # 记得梯度置零\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"iter #{epoch}: w={w.item():.4f}, l={l.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #0: w=2.0000, l=68.4709\n",
      "iter #10: w=2.0000, l=1.8042\n",
      "iter #20: w=2.0000, l=0.0774\n",
      "iter #30: w=2.0000, l=0.0310\n",
      "iter #40: w=2.0000, l=0.0281\n",
      "iter #50: w=2.0000, l=0.0264\n",
      "iter #60: w=2.0000, l=0.0249\n",
      "iter #70: w=2.0000, l=0.0234\n",
      "iter #80: w=2.0000, l=0.0221\n",
      "iter #90: w=2.0000, l=0.0208\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "Y = X * 2  # 注意: 这里举例不太好，是向量x使用标量w映射到向量y\n",
    "\n",
    "model = torch.nn.Linear(in_features=X.shape[1], out_features=Y.shape[1])\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-2)\n",
    "\n",
    "n_iter = 100\n",
    "for epoch in range(n_iter):\n",
    "    y_pred = model(X)\n",
    "    l = loss(Y, y_pred)  # 注意别写反顺序了\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"iter #{epoch}: w={w.item():.4f}, l={l.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "a71e63fc-0a04-47a9-8618-f206c5bbfbdd",
  "filePath": "/mnt/bn/tob-lq/qianweishuo/pytorchTutorial/notebooks/01_Installation.ipynb",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
